% In results, need to point out that for larger problems, reasonable
% discretizations into 100 intervals per variable could lead to
% (10^2)^6 = 10^12 = 1 trillion states that would prohibit the
% application of even one Bellman backup.  This solution is exact to
% arbitrary precision and does not introduce additional errors owing
% to discretization noise.
%
% Should mention somewhere modifications to max operator to handle
% negative values and important order of operations when working
% with infinities (0*anything=0) so cannot terminate early when
% see a 0 on a branch during Apply.
%
% Future work: hierarchy and decompositions with bounds, RTDP, 
%              UCT and sampling, determinization-discretization 
%              guided solutions, reward shaping, upper and lower
%              bounds and refining solutions, nonlinear by adaptive 
%              linearization, bounded approximations, extensions 
%              beyond piecewise linear to bilinear, quadratic, and 
%              beyond, conversion to polygons and GPU computation
%              -- or just keep polytopes -- key is parrallelization,
%              exploit more structure on min/max and integrate --
%              more efficient than for every path?  policy iteration
%              and finite state controllers; policy constraints
%              reduce points for expensive (continuous) maximization.
%              Concurrent multiagent zero-sum conditioned on 
%              continuous state.  Aync. VI by backing up state with
%              largest bound gap, or Bounded Cont. RTDP.
%              Can handle nonlinear with RTDP... instantiate state
%              prior to backup!!!

This work has combined symbolic techniques and data structures from
the HMDP literature in AI with techniques from chance-constrained
control theory to provide optimal robust solutions to a range of
problems with general continuous transitions and state-dependent noise
for which no general exact closed-form solutions previously existed.
Using these techniques we were able to find optimal policies and
answer questions of robust controllability for a variety of highly
risk-sensitive applications from AI planning, control theory, and
operations research such as \textsc{UAV Navigation}, \textsc{Space
  Telescope Control}, and \textsc{Reservoir Control}.  Among 
potential avenues for future work, combining this receding horizon
control approach with focused search techniques as in HAO*~\cite{hao09}
should preserve our strong robust optimality guarantees while
substantially increasing the scalability of our approach in exchange
for restricting solution optimality to a known set of initial states.
