
In order to compute the equations above, we propose a \emph{Robust symbolic dynamic programming} approach similar to ~\cite{sanner_uai11}.  This requires a general value iteration proposed in Algorithm~\ref{alg:vi} (\texttt{VI})
and a regression subroutine in Algorithm~\ref{alg:regress}
(\texttt{Regress}) %where we have omitted parameters $\vec{b}$ and $\vec{x}$ from $V$ and $Q$ to avoid notational clutter. 
%Note that the noise variables are defined and minimized in the 
Before this RSDP approach we introduce the appropriate representation of \emph{case} statements. All operations required for computing $Q$ and $V$ functions are \emph{case operations} which are defined next. 

\subsection{Case Representation and Operators}

Symbolic functions can generally be represented in \emph{case} form~\cite{fomdp}:
{%\footnotesize 
\begin{align}
f = 
\begin{cases}
  \phi_1: & f_1 \\ 
 \vdots&\vdots\\ 
  \phi_k: & f_k \\ 
\end{cases} \label{eq:case}
\end{align}
}
%how to put noise in here
where $\phi_i$ are logical formulae defined over the state
$(\vec{b},\vec{x})$ and can include arbitrary logical ($\land,\lor,\neg$)
combinations of boolean variables and \emph{linear} inequalities ($\geq,>,\leq,<$) 
over continuous variables.  
%Each $\phi_i$ will be disjoint from the other $\phi_j$ ($j \neq i$);  however the $\phi_i$ may not exhaustively cover the state space, hence $f$ may only be a \emph{partial function} and may be undefined for some variable assignments.

The $f_i$ may be either linear or quadratic in the continuous parameters with no discontinuities at partition boundaries. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\vspace{-.5mm}
\dontprintsemicolon
\SetKwFunction{regress}{Regress}
\Begin{
   $V^0:=0, h:=0$\;
   \While{$h < H$}{
       $h:=h+1$\;
       \ForEach {$a(\vec{y}) \in A$}{
              $Q_a^{h}(\vec{y},\vec{n})\,:=\,$\regress{$V^{h-1},a,\vec{y}$}\;
				$Q_a^{h}(\vec{y}) := \min_{\vec{n}} \, Q_a^{h}(\vec{y},\vec{n})$ $\,$\emph{//Stochastic $\min$}\;
				$Q_a^{h} := \max_{\vec{y}} \, Q_a^{h}(\vec{y})$ $\,$ \emph{// Continuous $\max$}\;
              $V^{h} := \casemax_a \, Q_a^{h}$ $\,$ \emph{// $\casemax$ all $Q_a$}\;
              $\pi^{*,h} := \argmax_{(a,\vec{y})} \, Q_a^{h}(\vec{y})$\;
       }
       \If{$V^h = V^{h-1}$}
           {break $\,$ \emph{// Terminate if early convergence}\;}
   }
     \Return{$(V^h,\pi^{*,h})$} \;
}
\caption{\footnotesize \texttt{VI}(CSA-MDP, $H$) $\longrightarrow$ $(V^h,\pi^{*,h})$ \label{alg:vi}}
\vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\incmargin{.5em}
\linesnumbered
\begin{algorithm}[t!]
\vspace{-.5mm}
\dontprintsemicolon
\SetKwFunction{remapWithPrimes}{Prime}
%\SetKwFunction{sumout}{sumout}


\Begin{
    $Q=$ \remapWithPrimes{$V$} $\,$ \emph{// All $b_i \to b_i'$ and all $ x_i \to x_i'$} \;
    \emph{// Continuous regression marginal integration}\\
    \For {all $x'_j$ in $Q$}{
         $Q := \int Q \otimes P(x_j'|\vec{b},\vec{b}',\vec{x},a,\vec{y},\vec{n}) \, d_{x'_j}$\;
    }
    \emph{// Discrete regression marginal summation}\\
    \For {all $b'_i$ in $Q$}{
         $Q := \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y}) \right]|_{b_i' = 1}$\\
         \hspace{8mm} $\oplus \left[ Q \otimes P(b_i'|\vec{b},\vec{x},a,\vec{y},\vec{n}) \right]|_{b_i' = 0}$\;
    }
    $Q := R(\vec{b},\vec{x},a,\vec{y}) \oplus (\gamma \otimes Q)$
     
     \emph{// $\max$-in noise variables}\\
     \For {all $n_k$ in $Q$}{
         $Q_a^{h}(\vec{y},\vec{n}) := \casemax_{n_k} \, ( Q, N(n_k, b'_i,x_j'))$ $\,$ \;
	}
    \Return{$Q$} \;
}
\caption{\footnotesize \texttt{Regress}($V,a,\vec{y}$) $\longrightarrow$ $Q$ \label{alg:regress}}
\vspace{-1mm}
\end{algorithm}
\decmargin{.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Unary operations} such as scalar multiplication $c\cdot f$ %(for a constant $c \in \mathbb{R}$) 
and negation $-f$ on case statements $f$ is performed by appling the operation to each
$f_i$ ($1 \leq i \leq k$). 
\emph{Binary operation} on two case statements are done by taking the cross-product
of the logical partitions of the two case statement and performing the
corresponding operation on the resulting paired partitions.  
The ``cross-sum'' $\oplus$ of two cases are defined as below:

{\footnotesize 
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-6mm} 
  $\begin{cases}
    \phi_1: & f_1 \\ 
    \phi_2: & f_2 \\ 
  \end{cases}$
$\oplus$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: & g_1 \\ 
    \psi_2: & g_2 \\ 
  \end{cases}$
&
\hspace{-2mm} 
$ = $
&
\hspace{-2mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1: & f_1 + g_1 \\ 
  \phi_1 \wedge \psi_2: & f_1 + g_2 \\ 
  \phi_2 \wedge \psi_1: & f_2 + g_1 \\ 
  \phi_2 \wedge \psi_2: & f_2 + g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
}
\normalsize

The other operations of $\ominus$ and $\otimes$ are performed by subtracting or multiplying partition values.  Note that some partitions may become inconsistent (infeasible)  which are removed from the final result. 

The other operations required for RSDP are restriction, substitution and maximization on case statements.  
\texttt{Regression} is defined separately for discrete and continuous variables.\emph{Boolean restriction} $f|_{b=v}$ assigns the value $v \in \{ 0,1 \}$ to any occurrence of $b$ in $f$. \emph{Continuous integration} such as $\int Q(x'_j) \otimes P(x'_j|\cdots) dx'_j$ in line 5 is performed symbolically to obtain $\int f(x'_j) \otimes \delta[x_j' - h(\vec{z})] dx'_j = f(x'_j) \{ x'_j / h(\vec{z}) \}$. Here $P(x_j'|\cdots)$ is in the form of $\delta[x_j' - h(\vec{z})]$ ($h(\vec{z})$ which is a case statement and $\vec{z}$ does not contain $x'_j$. Note that the latter operation in the result indicates that any occurrence of $x'_j$ in $f(x'_j)$ is \emph{symbolically substituted} with the case statement $h(\vec{z})$. Full specification of these two operation is defined in~\cite{sanner_uai11}. 

A \emph{symbolic case maximization} on two case statements is performed below:
\vspace{-4mm}

{\footnotesize
\begin{center}
\begin{tabular}{r c c c l}
&
\hspace{-7mm} $\casemax \Bigg(
  \begin{cases}
    \phi_1: \hspace{-2mm} & \hspace{-2mm} f_1 \\ 
    \phi_2: \hspace{-2mm} & \hspace{-2mm} f_2 \\ 
  \end{cases}$
$,$
&
\hspace{-4mm}
  $\begin{cases}
    \psi_1: \hspace{-2mm} & \hspace{-2mm} g_1 \\ 
    \psi_2: \hspace{-2mm} & \hspace{-2mm} g_2 \\ 
  \end{cases} \Bigg)$
&
\hspace{-4mm} 
$ = $
&
\hspace{-4mm}
  $\begin{cases}
  \phi_1 \wedge \psi_1 \wedge f_1 > g_1    : & \hspace{-2mm} f_1 \\ 
  \phi_1 \wedge \psi_1 \wedge f_1 \leq g_1 : & \hspace{-2mm} g_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 > g_2    : & \hspace{-2mm}f_1 \\ 
  \phi_1 \wedge \psi_2 \wedge f_1 \leq g_2 : & \hspace{-2mm} g_2 \\ 
 % \vdots & \vdots
 \phi_2 \wedge \psi_1 \wedge f_2 > g_1    : & \hspace{-2mm} f_2 \\ 
 \phi_2 \wedge \psi_1 \wedge f_2 \leq g_1 : & \hspace{-2mm} g_1 \\ 
 \phi_2 \wedge \psi_2 \wedge f_2 > g_2    : & \hspace{-2mm} f_2 \\ 
 \phi_2 \wedge \psi_2 \wedge f_2 \leq g_2 : & \hspace{-2mm} g_2 \\ 
  \end{cases}$
\end{tabular}
\end{center}
} If all $f_i$ and $g_i$ are linear,
the $\casemax$ result is clearly still linear.  If the $f_i$ or $g_i$
are quadratic (e.g. in the reward), then $f_i > g_i$ or $f_i \leq
g_i$ will be at most univariate quadratic which can be linearized into two linear inequalities. Hence the result of this $\casemax$
operator will be representable in the case format with linear inequalities in decisions.

For continuous maximization according to ~\cite{sdp_aaai} the $\max_y$ for each case partition
is computed individually that is $\max_y \phi_i(\vec{b},\vec{x},y) f_i(\vec{b},\vec{x},y)$.
In $\phi_i$ each conjoined constraint serves as either the upper bound on $y$, lower bound on $y$ or independent of $y$. The $\casemax$ ($\casemin$) operator is then used to determine the highest lower bound $\LB$
(lowest upper bound $\UB$) for multiple symbolic upper and lower bounds on $y$.
Apart from the lower and upper bound, the roots of $\frac{\partial}{\partial y} f_i$ 
w.r.t.\ $y$ are also potential maxima points. These points are then symbolically evaluated to find which yields the
maximizing value $\Max$.  Independent constraints and additional constraints that arise from the
symbolic nature of the $\UB$, $\LB$, and $\Root$ are also incorporated in the final result.
To complete the maximization for an entire case statement $f$, the
above procedure is applied to each case partition of $f$ and the continuous maximization on $f$ is the $\casemax$ of all individual results.  

To implement the case statements efficiently with continuous variables, extended Algebraic Decision diagrams (XADDs) are used from ~\cite{sanner_uai11} which is extended from ADDs ~\cite{bahar93add}. Unreachable paths can be pruned in XADDs using LP solvers and all operations including the continuous minimization explained in the next section can be defined using XADDs.
In the next section we show how to solve RDP symbolically using case statements.  

%The only operation that has not been previously defined for XADDs is $\max_y$, but this is easy: treating each XADD path from root to leaf node as a single case partition with conjunctive constraints,  $\max_y$ is performed at each leaf subject to these constraints and all path $\max_y$'s are then accumulated via the $\casemax$ operation to obtain the final result.

\subsection{Symbolic Robust Dynamic Programming}

In this section we use the RCSA-MDP and case statement definitions to define the optimal value function $V^h$ at horizon $h$ as defined in section 2.2. Algorithm ~\ref{alg:vi} (\texttt{VI}) outlines the main steps of a symbolic RDP approach.

Initially the value function $V^h$ is assigned to 0. For every horizon the $h$-stage-to-go value functions $V^h(\vec{b},\vec{x})$ is computed as described in the following.  
For every action, the function $Q^h_a$ is computed. Line 6 refers to Algorithm~\ref{alg:regress} which has the main steps below:
(\texttt{Regress}) which has the following steps: 
(i) Substituting the next states in the value function.
(ii) Performing Regression for continuous variable $x$ in line 5 and boolean variable $b$ in lines 8--9.
(iii) Multiplying the regression by the discount factor and adding the reward function.
(iii) Maximizing this result with the noise function in line 13. This step incorporates noise into the regressed Q-function consequently for each noise variable. Each noise variable assigns -$\infty$ for legal values inside the boundary range +$\infty$ for illegal values defined by the noise model $N(\vec{n},\vec{b},\vec{x})$.By maximizing in $n_k$ all illegal values will remain +$\infty$ since this is the maximum value compared to any other value and all legal values will be replaced by the regressed $Q$-value defined in step (iii) (-$\infty$ is less than any other $Q$-value so it is omitted in the maximization). 

Naturally a noisy process aims to minimize the noise in time to reach robustness Thus the regressed stochastic $Q_a^{h}(\vec{y},\vec{n})$ is now minimized over the noise variables $\vec{n}$ in line 7. Intuitively this continuous minimization will never choose +$\infty$ as there is always some value smaller which insures that the transitioned model never chooses illegal values. All legal $Q$-values are considered in the minimization step to find the value corresponding to the minimum noise.  
This continuous minimization is similar to that of continuous maximization explained in the previous section. The minima points of upper and lower bounds on $n_k$ are evaluated for the minimum possible value of noise. The $\LB \leq \UB$ constraint and the constraints independent of $n_k$ are also considered for the minimization on a single partition. The final result is a $\casemin$ on all the individual minimum results. 

%more on continuous minimization here with an example
The resulting Q-value with minimal noise is maximized over the continuous action parameter in line 8; a symbolic continuous maximization operation.  A discrete $\casemax$ on the set of discrete actions  for all $Q$-functions defines the final $V$ and the optimal policy is defined as the $\argmax$  over the set of discrete and continuous actions on $Q$. 