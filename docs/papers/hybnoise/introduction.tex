% Robust Optimization Hybrid MDPs

Many real-world sequential decision-making problems are naturally
modeled with both discrete and continuous (hybrid) state and action
spaces.  When state transitions are stochastic, these problems can be
modeled as Hybrid Markov Decision Processes (HMDPs), which have been
studied extensively in AI
planning~\cite{boyan01,feng04,li05,kveton06,phase07,hao09,sdp_aaai}
as well as control theory~\cite{Henzinger:1997,Hu:2000,DeSHee:2009}
and operations research~\cite{puterman}.  However, all previous
solutions to hybrid MDPs either take an approximation approach or
restrict stochastic noise on continuous transitions to be
state-independent or discretized (i.e., requiring continuous
transitions to be a finite mixture over deterministic transitions).
% Not mentioning initial state dependence of many control solutions,
% will mention these limitations and inability to handle
% controllability in Related Work.

Unfortunately, each of these assumptions can be quite limiting in
practice when strong \emph{a priori} guarantees on performance are
required in the presence of general forms of state-dependent noise.
For example, in a \textsc{UAV Navigation} problem~\cite{Blackmore:2011}, a human
controller must be aware of all positions from which a UAV with a
given amount of fuel reserves can return to its landing strip with
high probability of success given known areas of (state-dependent)
turbulence and weather events.  In a \textsc{Space Telescope Control}
problem~\cite{DLohr:2012}, one must carefully manage inertial moments and
rotational velocities as the telescope maneuvers between different
angular orientations and zoom positions, where noise margins increase
when the telescope is in unstable positions (extended zooms).  And
in a \textsc{Reservoir Control} problem, one must manage
reservoir levels to ensure a sufficient water supply for a population
while avoiding overflow conditions subject to uncertainty over daily
rainfall amounts.  In all of these problems, there is no room for
error: a UAV crash, a space telescope spinning uncontrollably, or a
flooded reservoir can all cause substantial physical, monetary, and/or
environmental damage.  What is needed are robust solutions to these
problems that are cost-optimal while guaranteed not to exceed a
prespecified margin of error.

To achieve cost-optimal robust solutions we build on ideas
used in the chance-constrained control
literature~\cite{Schwarm:1999,Li:2002,Ono:2008,Blackmore:2011} that
maintain confidence intervals on (multivariate) noise distributions
and ensure that all reachable states are within these noise margins.
However, previous methods restrict either to linear systems with
Gaussian uncertainty and state-independent noise or resort
to approximation techniques.  Furthermore, as these works are all
inherently focused on control from a given initial state, they are
unable to prove properties such as \emph{robust controllability},
i.e., what states have a policy that can achieve a given cost with
high certainty over some horizon?

In this work, we adopt a robust optimization receding horizon control
approach in which Nature is allowed to adversarially determine
transition noise w.r.t. constrained non-deterministic transitions in HMDPs.
This permits us to find optimal robust solutions for a wide range of
non-deterministic HMDPs and allows us to answer questions of \emph{robust
controllability} in very general state-dependent continuous noise settings.
Altogether, this work significantly extends previous results in both
the HMDP literature in AI and robust hybrid control literature
and permits the solution of a new class of robust HMDP control problems.

% What is currently missing are technical claims of novelty: in fact
% all previous operations existed so it's really just the observation
% that they can be combined and used for robust control

