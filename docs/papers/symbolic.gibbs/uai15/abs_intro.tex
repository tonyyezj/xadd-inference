\begin{abstract}
Probabilistic inference in many real-world problems often requires
graphical models with (1)~nonlinear deterministic constraints between
random variables (e.g., laws of physics) and (2)~piecewise
distributions (e.g., priors enforcing bounded parameter values).
While Gibbs sampling provides an attractive asymptotically unbiased
MCMC approximation approach that does not require proposal design or
tuning, it cannot be directly applied to models with determinism in
the case of (1) and is difficult to automate in the case of (2).  To
address both limitations, we introduce a rich class of piecewise
algebraic graphical models with nonlinear determinism, where we show
that deterministic constraints can always be eliminated, thus permitting the
application of Gibbs sampling; we further show the collapsed model always
permits one symbolic integral --- sufficient to
automatically derive conditionals for Gibbs sampling.  We evaluate
this fully automated Symbolic Gibbs sampler on models motivated by physics and
engineering and show it converges an order of magnitude faster than
existing Monte Carlo samplers.

\begin{comment}
furthermore
automatic derivation of conditional distributions required for 
Gibbs samplers 

for arbitrary piecewise
distributions

, rejection sampling is often inefficient in practice
and Metropolis-Hastings often requires manual tuning of a  both prove inefficient in practice, and
analytical derivation of Gibbs samplers

To date, the inference tools in such models have largely focused on Markov Chain
Monte Carlo methods due to their guarantees of asymptotically unbiased inference



In many applications of probabilistic reasoning, 
instead of stochastic random variables, 
functions i.e.\ deterministic relationships between them
might be observed.
Beyond some restricted settings, effective inference conditioned on such relationships has remained an open problem. % so far.
%
The most commonly suggested solution is to approximate (i.e.\ soften) the determinism by adding noise to the 
observation. %observed constraints.
In practice, however, this strategy may 
lead to extremely low mixing rates and/or arbitrarily large approximation bounds. 
%not help much:
%In the presence of small additive noise, the mixing rate of Monte Carlo algorithms notoriously deteriorates 
%%If the added noise is small, the mixing rate of Monte Carlo algorithms notoriously deteriorates 
%while by adding large noise, the approximation bounds may become arbitrarily large.
%%while large noise leads to arbitrarily large approximation bounds.
%
We fill this gap by contributing a variation of Gibbs sampling, called \emph{symbolic Gibbs}, 
that carries out effective and automatic inference conditioned on a large range of (non)linear
deterministic relationships in a rich family of distributions not studied in the literature so far.
%We fill this gap by contributing a novel framework which, for the first time, carries out inference conditioned on a large range of (non)linear deterministic constraints in a highly expressive class of distributions that is not studied in the literature so far.
% Our other contribution is to provide an effective MCMC inference technique  based on the insight that in our model, most costly operations required for Gibbs sampling can be accomplished analytically and offline, prior to sampling (\emph{symbolic Gibbs}).
We evaluate this algorithm on models in physics and engineering and show that it is an order of magnitude faster than the baseline Gibbs sampler and a significant improvement over other Monte Carlo methods.  
\end{comment}
\end{abstract}

\section{Introduction}
\label{sect:intro}

Probabilistic inference in many real-world problems often requires
graphical models with nonlinear deterministic constraints between
random variables (e.g., laws of physics) and piecewise
distributions (e.g., priors enforcing bounded parameter values).
To illustrate this, let us consider the following example:

\begin{comment}
Graphical models (GMs) are the lingua franca for probabilistic reasoning.
They denote the conditional dependence structure between random variables by directed or undirected graphs \citep{koller2009probabilistic}. 
A (random) variable is deterministic if its conditional distribution has zero variance. 
%Unobserved deterministic random variables that is, variables that cannot be given data or initial values (corresponding to \emph{logical nodes} in BUGS software) almost only provide notational convenience.
Observed deterministic variables (or \emph{observed determinism}, for short) represent deterministic dependencies on other variables.
Such constraints can appear in a variety of real-world applications.
For instance, instead of direct observation of random variables 
$X_1, X_2, \ldots$, a function of them $f(X_1, X_2, \ldots)$ might be observed. 
%such deterministic relationships between random variables might be governed by natural laws (such as Kirchhoff's circuit laws or Newtonian mechanics).
To motivate the discussion consider a concrete example:  
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\vspace{-3mm}
\begin{center}
%\begin{tabular}{cc}
 %  \hspace{-0mm} \includegraphics[width=0.17\linewidth]{Figs/little-momentum0.pdf} 
 \includegraphics[width=0.33\linewidth]{Figs/little-momentum1.pdf} 
%\vspace{-0.05mm}
%\\
%\hspace{-5mm} \footnotesize(a) 
%& \hspace{-3mm} \footnotesize(b)  \\
%\multicolumn{2}{c}{}
%\end{tabular}
\end{center}
\vspace{-4mm}
\caption{\footnotesize
Bayesian network for the \emph{collision model}. 
Shaded circles indicate variables that are results of deterministic functions.} 
\label{fig:mom0}
\vspace{-2mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%\vspace{1mm}\\
{\bf Example (Collision model). }
\emph{Masses $M_1$ and $M_2$ with velocities $V_1$ and $V_2$ (and consequently momenta $P_1 = M_1 V_1$ and $P_2 = M_2 V_2$) collide to form a single mass ($M_1 + M_2$) with momentum $P_\text{tot} = P_1 + P_2$ (assuming that there is no dissipation).
The masses and velocities are unknown but 
their prior distributions are given as follows:\footnote{
$\mathcal{U}(a, \, b)$ denotes a uniform distribution on interval $[a,b]$.
%with support interval $[a, b]$.
} 
%(note that as Figure~\ref{fig:mom0}.a shows, a collision only happens if $V_2 < V_1$):  
}%end \emph
{\footnotesize %\vspace{-0.5mm}
\begin{align}
&\pr(M_1) = \mathcal{U}(0.1, \, 2.1) 
&&\pr(M_2) \!=\! \mathcal{U}(0.1, \, 2.1)
\nonumber
\\
&\pr(V_1) = \mathcal{U}(-2, \, 2)
&&\pr(V_2 \, | \, V_1) = \mathcal{U}(-2, \, V_1)
\label{e:collision}
\end{align} 
}
\emph{
\!\!\!Conditional dependencies are illustrated in Figure~\ref{fig:mom0}. %b.
We wish to infer $\pr(M_1, M_2, V_1, V_2 | P_{\text{tot}} = 3)$.
%Conditioned on the observation $P_{\text{tot}} = 3$, the posterior joint distribution of $M_1$, $M_2$, $V_1$ and $V_2$ is desired. 
\hspace*{\fill} $\diamond$} %\emph
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%

We note this model involves both deterministic nonlinear constraints
(e.g., $P_1 = M_1 V_1$) as well as piecewise distributions (e.g.,
$\pr(M_1) = \mathcal{U}(0.1, \, 2.1)$).  Unfortunately, exact
closed-form inference in models such as these is impossible in the
general case.  While the Markov Chain Monte Carlo method of Gibbs
sampling provides an attractive asymptotically unbiased inference
approach that does not require proposal design or tuning, it cannot be
directly applied to models with determinism nor can the integrals
required for deriving conditional distributions be symbolically
computed for general piecewise functions.

\begin{comment}
%% No room for discussion in this work...
Despite its humble appearance, we will show the solutions that the existing probabilistic inference tools provide for this model are quite unsatisfactory. 

Let's take a closer look on the nature of the problem by manually
carrying on the computations required for \emph{Markov chain Monte
  Carlo} (MCMC) sampling: It can easily be seen that in the
\emph{collision model}, the joint density of masses and velocities is
\begin{equation} \footnotesize  
\label{e:col-prior}
\pr(M_1, M_2, V_1, V_2)  
=
\begin{cases}
\frac{1}{16 V_1 + 32} &{\text{if }\scriptstyle 0.1<M_1<2.1, \, 0.1<M_2<2.1,}\\
							 &{\;\;\, \scriptstyle -2<V_1<2, \, -2<V_2 < V_1}\\
 \otherwise{0}
 \end{cases}
\end{equation}
which is a 4 dimensional function from which samples can be taken fairly easily. 
Since {\footnotesize $P_\text{tot} = M_1 V_1 + M_2 V_2$}, 
the following \emph{likelihood} is a 0-variance conditional distribution:
\begin{equation}\footnotesize 
\label{e:col-likelihood}
\pr(P_\text{tot} = 3 \,|\, M_1, V_1, M_2, V_2) = \delta( M_1 V_1 + M_2 V_2 - 3)
\end{equation}
where $\delta(\cdot)$ denotes the \emph{Dirac delta}. 
By \emph{Bayes} rule, 
the \emph{posterior} 
{\footnotesize $\pr(M_1, M_2, V_1, V_2 \,|\, P_\text{tot} = 3)$}
is proportional to the product of equations (\ref{e:col-prior}) and (\ref{e:col-likelihood}).
Note that despite being 4-dimensional, 
the mass of this function is only non-zero on the 3 dimensional \emph{hypersurface}: {\footnotesize$M_1 V_1 + M_2 V_2 = 3$}. 


In general, observation of an algebraic relationship between random variables of an {\footnotesize$N$}-dimensional prior space often corresponds to a zero-probability event that leads to a posterior mass positioned on an {\footnotesize$(N\!-\!1)$}-dimensional hyperspace and conditioning on multiple such events increases the difference between the dimensionality of the original space and the \emph{submanifold} on which the posterior is non-zero.  
If this dimensionality mismatch is not taken into account, sampling from such posterior distributions will be impossible. (As a more tangible example, note that taking samples from a 2D curve via random walk in a 3D continuous space is impossible.)  
\end{comment}

To address both of these limitations of Gibbs sampling, we introduce a
rich class of piecewise algebraic graphical models with nonlinear
determinism, including models like the \emph{collision} example.  We
show that deterministic constraints can always be eliminated by
collapsing out one variable involved in the constraint (even when this
variable is involved in non-invertible functions) thus permitting the
application of Gibbs sampling; furthermore, the collapsed model always
permits at least one symbolic integration, which is sufficient to
automatically derive the conditionals necessary for Gibbs sampling.
We evaluate this fully automated Symbolic Gibbs sampler on scalable
domains motivated by physics and engineering, showing that it
converges an order of magnitude faster than existing Monte Carlo
samplers.

\begin{comment}

%We are unaware of any off-the-shelf inference framework that handles conditioning on deterministic constraints among random variables.
\subsection{Literature review}
By imposing various %(but functionally the same)
\emph{syntactical restrictions}, % and excuses, 
the state-of-the-art probabilistic programming languages (PPLs),
%totally 
disallow deterministic relationships among continuous random variables 
(a.k.a.\ deterministic constraints) be observed.\footnote{
In BUGS \citep{lunn2009bugs}, the popular software for analysis of statistical models, \emph{logical nodes} cannot be given data or initial values .
In PyMC \citep{patil2010pymc} deterministic variables have no \emph{observed flag}. 
In Stan \citep{stan-manual:2014} 
if you try to assign an observation value to a deterministic variable, you will encounter an error message: 
``attempt to assign variable in wrong block'' while 
Anglican \citep{wood2014new} throws error ``invalid-observe", etc.}

The solution that these off-the-shelf inference frameworks suggest is to approximate the observed determinism via adding noise to the observation 
(hard to soft constraint conversion via introducing measurement error).

For example the r.h.s of equation (\ref{e:col-likelihood}) would be 
approximated with a normal distribution
{\footnotesize $\mathcal{N}( M_1 V_1 + M_2 V_2 - 3, \sigma_\eta^2)$}  
where the variance $\sigma_\eta^2$ is the noise parameter.

Using this trick, it is guaranteed that the posterior is equidimensional with the (space of) prior (i.e.\ its ambient space) and therefore in theory, conventional (MC)MC sampling is possible.
Nonetheless, in practice such an strategy may not help much:
If the added noise is large, 
the approximation bounds may become arbitrarily large
and if the noise is small, 
the mixing rate may become extremely slow \citep{chin1987bayesian}. 
The reason is that in the latter case, 
 the approximated posterior mass would be dense in tinny regions (surrounding the original submanifold) on which random walk
is notoriously inefficient (near-deterministic problem).  

%If all stochastic distributions of the model belong to particular families of density functions, the problem may be addressed by means of  
%\emph{variable transformations}.
In particular settings, \emph{linear} deterministic relationships between continuous random variables
are addressed in the literature.  
Most notably, 
the family of \emph{conditional linear Gaussians (CLGs)} \citep{lauritzen2001stable}, 
i.e\ constant-variance \emph{Gaussian} densities where the mean of each random variable is a linear function of its parents,
is closed under linear transformations and consequently
allow linear deterministic constraints.%\footnote{CLGs form constant-variance \emph{Gaussian} Bayesian networks where the mean of each stochastic random variable is a linear function of its parents. \citep{lauritzen2001stable}.}

In \citep{li2013dynamic} the restriction is imposed on the network topology.
They use \emph{sequential importance sampling} on a simple Bayesian network of i.i.d. data points with an observed summation. % in a minimalistic BN.
%They conjecture that such an algorithm may be generalized to other settings. 
The generalization of this technique does not seem straight-forward (if possible).
%It is unknown if this technique can be generalized to other topologies and evidence. 

Recently, piecewise polynomial distributions have attracted attention.
So far, piecewise polynomials with hyperrectangular, hyper-rhombus and linear partitioning constraints have been used in Bayesian networks \citep{shenoy2011inference,shenoy2012two,Sanner:12}.
Such families can handle linear deterministic conditionals.
\cite{cobb2005nonlinear} approximate non-linear deterministic constraints with piecewise linear constraints via
dividing the domain of the parent variables into hypercubes. % and creating an approximation of each function in each hypercube.
This method is not scalable and rapidly loses its performance in high dimensions since 
the number of partitions required to preserve approximation accuracy
grows exponentially in the variable space dimensionality. 

In short, inference conditioned on nonlinear deterministic relationships between continuous random variables is still an open problem \citep{li2013dynamic} and as mentioned, even for linear constraints, the existing works are too restrictive.
 
\subsection{Contribution and outline}
In this paper, we provide an efficient automatic inference mechanism 
on an expressive class of continuous distributions 
that handles a large class of linear and nonlinear multivariate algebraic deterministic relationships between continuous random variables that are in the form of polynomial fractions. 

After a brief background to %representation and inference in 
graphical models (GMs) in Section~\ref{sect:background}, 
%our first main contributions is presented in Section~\ref{sect:contribution1} where 
in Section~\ref{sect:contribution1} we provide the necessary theory and algorithm (called \emph{collapsing determinism}) 
to transform models with a large class of (observed) deterministic constraints to purely stochastic networks. % for a large set of (non)linear algebraic determinism. 
%The main idea is to reduce the posterior dimensionality by utilizing the properties of Dirac delta distributions. This algorithm relies on operations such as fractional substitutions. It can easily be seen that under such operations, the typical classes of distributions that are studied in the literature so far do not remain closed. This should be the main reason why the problem of inference conditioned on determinism has remained {\color{green}untackled} so far. 
It can be seen that the classes of distributions studied in the literature so far do not remain closed under operations (such as fractional substitutions) required for collapsing determinism.
This is remedied in 
%Our second contribution is presented in 
Section~\ref{sect:ppfs} where we 
%{\color{green}alleviate} this situation as follows:\bf We 
introduce \emph{polynomial piecewise fractions} (PPFs), a highly expressive class of functions, not studied in the literature so far, 
with interesting properties that facilitate the inference conditioned on deterministic algebraic constraints.

%This class (called PPFs for \emph{polynomial piecewise fractions}) {\color{green}consists of} piecewise functions where the space of variables is partitioned by polynomial inequalities and the the function value in each partition is a fraction with a polynomial numerator and denominator.  
%Clearly, PPFs can approximate any function up to arbitrary precision by partitioning it into (relatively) {\color{green} homogeneous} pieces followed by utilizing \emph{Taylor} expansion or similar methods.
%The key insight is that PPFs are closed under derivation and polynomial fractional substitutions. This immediately extends their application to models with sophisticated algebraic constraints.

%Unfortunately, PPFs are not closed under integration (required for marginalization). Therefore, closed-form inference on PPF models is not in general possible.  Nonetheless, 

A large subset of PPFs have closed-form univariate symbolic integrals which leads to another contribution
presented in Section~\ref{sect:symbolic.gibbs}: %our third contribution: In Section~\ref{sect:symbolic.gibbs}, we propose 
performing 
the costly operation of univariate CDF computation, as required by Gibbs sampling, analytically and offline rather than per sample.     
%Gibbs samplers are robust in the sense that they do not need any parameter tuning.However, they are computationally expensive since to take a single sample they require to compute many univariate integrals (to generate the CDFs of conditional distributions).
%We notice that a large subset of PPFs have closed form (symbolic) integrals. 
%The idea of Symbolic Gibbs is to compute a single analytic integral for each variable by keeping the remaining variables uninstantiated. 
%The actual process of CDF computation per sample  simply involves instantiation of the already computed CDFs rather than computing them on the fly.
This dramatically decreases the amount of necessary computations and as our experimental results in Section~\ref{sect:experimental.results} show, 
leads to a significant improvement in the convergence rates. 
%Literature review is postponed to Section~\ref{sect:literature.review} and 
The summary and conclusion are offered in Section~\ref{sect:conclusion}. 

%In the following sections after a brief introduction to Graphical models, followed by an algorithm for inference in the presence of deterministic constraints.We then explain the restrictions of PPFs, compare our method with other samplers and conclude. 
\end{comment}
