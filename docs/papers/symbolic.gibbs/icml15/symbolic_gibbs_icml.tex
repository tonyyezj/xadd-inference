%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern

%H
%\usepackage{caption}
%\usepackage{subcaption}

%\usepackage{epsfig} % less modern
%\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath,amsfonts,amssymb,amsthm}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Symbolic Gibbs Sampling in Piecewise Algebraic Graphical Models with Nonlinear Deterministic Constraints}

\usepackage{verbatim}
%\usepackage{ntheorem} % framed and list of theorems [standard,framed,thref] \listtheorems{types}		%COMMENTED BY HADI
\usepackage{color}    % color
\newtheorem{theorem}{Theorem}

%OLD
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ind}[1]{\mathbb{I}[#1]}
\newcommand{\inde}{\mathbb{I}}

\newcommand{\var}{v}
\newcommand{\eq}{\leftarrow}

\newcommand{\LB}{\mathit{LB}}
\newcommand{\UB}{\mathit{UB}}

\newcommand{\B}{\mathbb{B}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\F}{\mathbb{F}}
\renewcommand{\vec}[1]{\mathbf{#1}}

%NEW:
\newcommand{\tuple}[1] {\langle #1 \rangle}
\newcommand{\bvec}[1]{\textbf{#1}}
\newcommand{\indicator}{\mathbb{I}}%{I\!\!I}
\newcommand{\case}[2]{#2 &\text{ if } #1}%{#1 : #2}
\newcommand{\singlecase}[2]{#2 \quad \text{ if } #1}
\newcommand{\otherwise}[1]{#1 &\text{ otherwise}}
\newcommand{\pr}{p}
\newcommand{\nn}{0.16}
\newcommand{\nnn}{0.33}
\newcommand{\nnh}{0.23}
\newcommand{\dd}{\;\mathrm{d}} % d for integration variables e.g. dx


\begin{document} 

\twocolumn[
\icmltitle{Symbolic Gibbs Sampling in Piecewise Algebraic Graphical Models with Nonlinear Deterministic Constraints}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Deterministic, evidence, relations, constraints, random variables, Symbolic, Analitical, Gibbs sampling}

\vskip 0.3in
]

\begin{abstract}
In many applications of probabilistic reasoning, 
instead of stochastic random variables, 
functions i.e.\ deterministic relationships between them
might be observed.
%A few families of distributions are closed under linear transformations and therefore, can handle observed linear deterministic relationships among random variables. However, beyond this limit, conditioning on deterministic functions of continuous random variables has remained an open problem so far.
Beyond some restricted settings, exact or approximate inference conditioned on such relationships has been remained an open problem so far.
\\ 
The most commonly suggested solution is to approximate (i.e.\ soften) the determinism by adding noise to the observed constraints.
In practice, however, this strategy may not help much:
If the added noise is small, the mixing rate of Monte Carlo algorithms notoriously deteriorates 
while by adding large noise, the approximation bounds may become arbitrarily large.
\\
We fill this gap by contributing a novel framework which,
for the first time, carries out inference conditioned on a large range of (non)linear deterministic constraints
in a highly expressive class of distributions that is not studied in the literature so far.
%Our experiments show that conditioned on determinism, the performance of Gibbs sampling surpasses other MCMC methods. 
Our other contribution is to provide an effective MCMC inference technique  
based on the insight that in our model, most costly operations required for Gibbs sampling can be accomplished analytically and offline, prior to sampling (\emph{symbolic Gibbs}).
We evaluate this algorithm on models in physics and engineering and show that it is an order of magnitude faster than the baseline Gibbs sampler and a significant improvement over other Monte Carlo methods.  
\end{abstract}

\section{Introduction}
\label{sect:intro}
Graphical models (GMs) are the lingua franca for probabilistic reasoning.
They denote the conditional dependence structure between random variables by directed or undirected graphs \cite{koller2009probabilistic}. A (random) variable is deterministic if its conditional distributions have zero variances. 
%Unobserved deterministic random variables that is, variables that cannot be given data or initial values (corresponding to \emph{logical nodes} in BUGS software) almost only provide notational convenience.
Observed deterministic variables represent deterministic dependencies on other variables.
Such constraints can appear in a variety of real-world applications.
For instance, instead of direct observation of random variables 
$X_1, X_2, \ldots$, a function of them $f(X_1, X_2, \ldots)$ might be observed. 
%such deterministic relationships between random variables might be governed by natural laws (such as Kirchhoff's circuit laws or Newtonian mechanics).
To motivate the discussion consider a concrete example:  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
\vspace{-3mm}
\begin{center}
\begin{tabular}{cc}
   \hspace{-0mm} \includegraphics[width=\linewidth/6]{Figs/little-momentum0.pdf} 
& \hspace{6mm} \includegraphics[width=0.35\linewidth]{Figs/little-momentum1.pdf} 
\vspace{-1.5mm}
\\
\hspace{-5mm} \footnotesize(a) 
& \hspace{-3mm} \footnotesize(b)  \\
\multicolumn{2}{c}{}
\end{tabular}
\end{center}
\vspace{-6mm}
\caption{\footnotesize
(a) Collision of masses $M_1$ and $M_2$ with velocities $V_1$ and $V_2$ and momenta $P_1$ and $P_2$. 
%(a) Collision happens if and only if $V_1>V_2$. 
(b) Corresponding Bayesian network. The filled circles represent deterministic random variables.} 
\label{fig:mom0}
\vspace{-7mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%\vspace{1mm}\\
{\bf Example (Collision model). }
\emph{Masses $M_1$ and $M_2$ with velocities $V_1$ and $V_2$ (and consequently momenta $P_1 = M_1 V_1$ and $P_2 = M_2 V_2$) collide to form a single mass ($M_1 + M_2$) with momentum $P_3 = P_1 + P_2$ (assuming that there is no dissipation).
Masses and velocities are unknown but 
their prior distributions are given as follows\footnote{
Throughout, $\mathcal{U}(a, \, b)$ denotes a uniform continuous distribution 
with support interval $[a, b]$.
} 
(note that as Figure~\ref{fig:mom0}.a shows, a collision only happens if $V_2 < V_1$):  
}%end \emph
{\footnotesize \vspace{-0.5mm}
\begin{align}
&\pr(M_1) = \mathcal{U}(0.1, \, 2.1) 
&&\pr(M_2) \!=\! \mathcal{U}(0.1, \, 2.1)
\nonumber
\\
&\pr(V_1) = \mathcal{U}(-2, \, 2)
&&\pr(V_2 \, | \, V_1) = \mathcal{U}(-2, \, V_1)
\label{e:collision}
\end{align} 
}
\emph{
\!\!\!The conditional dependencies of these random variables are illustrated in 
Figure~\ref{fig:mom0}.b.
Conditioned on the observation $P_3 = 3$, the posterior joint distribution of $M_1$, $M_2$, $V_1$ and $V_2$ is desired. 
\hspace*{\fill} $\diamond$} %\emph

Despite its humble appearance, we will show the solutions that the existing probabilistic inference tools provide for this model are quite unsatisfactory. 

Let's take a closer look on the nature of the problem by manually carrying on the computations required for MCMC sampling:
It can easily be seen that in the \emph{collision model}, 
the \emph{prior} joint probability of 
{\footnotesize $M_i$} and 
{\footnotesize $V_i$} is:
\begin{equation} \footnotesize  
\label{e:col-prior}
\pr(M_1, M_2, V_1, V_2)  
=
\begin{cases}
\frac{1}{16 V_1 + 32} &{\text{if }\scriptstyle 0.1<M_1<2.1, \, 0.1<M_2<2.1,}\\
							 &{\;\;\, \scriptstyle -2<V_1<2, \, -2<V_2 < V_1}\\
 \otherwise{0}
 \end{cases}
\end{equation}
which is a 4 dimensional function from which samples can be taken fairly easily. 
Since {\footnotesize $P_3 = M_1 V_1 + M_2 V_2$}, 
the following \emph{likelihood} is a 0-variance conditional distribution:
\begin{equation}\footnotesize 
\label{e:col-likelihood}
\pr(P_3 = 3 \,|\, M_1, V_1, M_2, V_2) = \delta\big( 3 - (M_1 V_1 + M_2 V_2) \big)
\end{equation}
where $\delta(\cdot)$ denotes \emph{Dirac delta}. 
By \emph{Bayes} rule, 
the \emph{posterior} 
{\footnotesize $\pr(M_1, M_2, V_1, V_2 \,|\, P_3 = 3)$}
is proportional to the product of equations (\ref{e:col-prior}) and (\ref{e:col-likelihood}).
Note that despite being 4-dimensional, 
the mass of this function is only non-zero on the 3 dimensional \emph{hypersurface}: {\footnotesize$M_1 V_1 + M_2 V_2 = 3$}. 


In general, observation of an algebraic function of an $N$-dimensional prior often leads to a posterior mass positioned on an $N-1$-dimensional hyperspace and existence of more deterministic constraints/relationships between stochastic random variables increases the difference between the dimensionality of the original space and the \emph{manifold} on which the posterior is non-zero.  
If this dimensionality mismatch is not taken into account, sampling from such posterior distributions via sampling is impossible (As a more tangible example, note that taking sample from a 2D curve via random walk in a 3D continuous space is impossible.)  

%We are unaware of any off-the-shelf inference framework that handles conditioning on deterministic constraints among random variables.
By imposing various %(but functionally the same)
\emph{syntactical restrictions},% and excuses, 
the state-of-the-art probabilistic programming languages (PPLs),
totally disallow deterministic relationships among continuous random variables be observed.
\footnote{
In BUGS \cite{lunn2009bugs}, the popular software for analysis of statistical models, \emph{logical nodes} cannot be given data or initial values .
In PyMC \cite{patil2010pymc} deterministic variables have no \emph{observed flag}. 
If in Stan \cite{stan-manual:2014} 
you try to assign an observation value to a deterministic variable, you will encounter an error message: 
``attempt to assign variable in wrong block'' while 
Anglican \cite{wood2014new} throws error ``invalid-observe", etc.}

The solution that these off-the-shelf inference frameworks suggest is to approximate the observed deterministic relations among  stochastic random variables via adding noise to the observation 
(hard to soft constraint conversion via introducing measurement error).

For example the r.h.s of equation (\ref{e:col-likelihood}) would be 
approximated with a normal distribution
{\footnotesize $\mathcal{N}(3 - M_1 V_1 - M_2 V_2, \sigma_\eta)$}  
where the standard deviation $\sigma_\eta$ is the noise parameter.

Using this trick, it is guaranteed that the posterior is equidimensional with the (space of) prior (i.e.\ its ambient space) and therefore in theory, conventional MCMC sampling is possible.
Nonetheless, in practice such an strategy may not help much:
If the added noise is large, 
the approximation bounds may become arbitrarily large
and if the noise is small, 
the mixing rate may become extremely slow \cite{chin1987bayesian}. 
The reason is that in the latter case, 
 the approximated posterior mass would be dense in tinny regions (surrounding the original manifold) on which random walk
is notoriously inefficient (near-deterministic problem).  

%If all stochastic distributions of the model belong to particular families of density functions, the problem may be addressed by means of  
%\emph{variable transformations}.
In particular settings, \emph{linear} deterministic conditionals of continuous random variables
are addressed in the literature.  
Most notably, 
the family of \emph{conditional linear Gaussians (CLG)} \cite{lauritzen2001stable}
is closed under linear transformations and consequently
allow linear deterministic constraints.\footnote{CLGs form constant-variance \emph{Gaussian} Bayesian networks where the mean of each stochastic random variable is a linear function of its parents. \cite{lauritzen2001stable}.
}

In \cite{li2013dynamic} the restriction is imposed on the network topology.
They use \emph{sequential importance sampling} on a simple Bayesian network of i.i.d. data points with an observed summation.% in a minimalistic BN.
%They conjecture that such an algorithm may be generalized to other settings. 
The generalization of this technique does not seem straight-forward (if possible).
%It is unknown if this technique can be generalized to other topologies and evidence. 

Recently, piecewise polynomial distributions have attracted attention.
So far, piecewise polynomials with hyperrectangular, hyper-rhombus and linear partitioning constraints have been used in Bayesian networks \cite{shenoy2011inference,shenoy2012two,Sanner:12}.
Such families can handle linear deterministic conditionals.
\cite{cobb2005nonlinear} approximate non-linear deterministic constraints with piecewise linear constraints via
dividing the domain of the parent variables into hypercubes and creating an
approximation of each function in each hypercube.
This method is not scalable and rapidly loses its performance in high dimensions since to preserved approximation accuracy, the number of partitions grow exponentially in the variable space dimensionality. 

In short, inference conditioned on nonlinear deterministic constraints on continuous random variables is still an open problem \cite{li2013dynamic} and as mentioned, even for linear constraints, the existing works are too restrictive.
 
In this paper, we tackle this problem for a large class of linear and nonlinear multivariate algebraic deterministic constraints in the form of polynomial fractions on an expressive class of continuous distributions. 

After a brief background for representation and inference in Graphical models (GMs) in Section~\ref{sect:background}, 
our first main contributions is presented in Section~\ref{sect:contribution1}
where we provide the necessary theory and algorithm (called \emph{collapsing determinism}) 
to convert GMs with deterministic and stochastic nodes into purely stochastic networks for a large set of (non)linear algebraic determinism. 

%The main idea is to reduce the posterior dimensionality by utilizing the properties of Dirac delta distributions. This algorithm relies on operations such as fractional substitutions. It can easily be seen that under such operations, the typical classes of distributions that are studied in the literature so far do not remain closed. This should be the main reason why the problem of inference conditioned on determinism has remained {\color{green}untackled} so far. 

It can be seen that the classes of distributions studied in the literature so far do not remain closed under operations (such as fractional substitutions) required for collapsing determinism.

Our second contribution is presented in Section~\ref{sect:ppfs} where we 
%{\color{green}alleviate} this situation as follows:\bf We 
introduce \emph{polynomial piecewise fractions}(PPFs), an extremely expressive class of functions, not studied in the literature so far, 
with interesting properties that facilitate the inference conditioned on deterministic algebraic constraints.

%This class (called PPFs for \emph{polynomial piecewise fractions}) {\color{green}consists of} piecewise functions where the space of variables is partitioned by polynomial inequalities and the the function value in each partition is a fraction with a polynomial numerator and denominator.  
%Clearly, PPFs can approximate any function up to arbitrary precision by partitioning it into (relatively) {\color{green} homogeneous} pieces followed by utilizing \emph{Taylor} expansion or similar methods.
%The key insight is that PPFs are closed under derivation and polynomial fractional substitutions. This immediately extends their application to models with sophisticated algebraic constraints.

%Unfortunately, PPFs are not closed under integration (required for marginalization). Therefore, closed-form inference on PPF models is not in general possible.  Nonetheless, 

A large subset of PPFs have closed-form univariate symbolic integrals which lead to our third contribution:
In Section~\ref{sect:symbolic.gibbs}, we propose performing the costly operation of univariate CDF computation, as required by Gibbs sampling, analytically and offline rather than per sample.     

%Gibbs samplers are robust in the sense that they do not need any parameter tuning.However, they are computationally expensive since to take a single sample they require to compute many univariate integrals (to generate the CDFs of conditional distributions).
%We notice that a large subset of PPFs have closed form (symbolic) integrals. 
%The idea of Symbolic Gibbs is to compute a single analytic integral for each variable by keeping the remaining variables uninstantiated. 
%The actual process of CDF computation per sample  simply involves instantiation of the already computed CDFs rather than computing them on the fly.
This dramatically decreases the amount of necessary computations and as in Section~\ref{sect:experimental.results} the experimental results show, leads to a significant speed improvement. 
%Literature review is postponed to Section~\ref{sect:literature.review} and 

The summary and conclusion are offered in Section~\ref{sect:conclusion}. 

%In the following sections after a brief introduction to Graphical models, followed by an algorithm for inference in the presence of deterministic constraints.We then explain the restrictions of PPFs, compare our method with other samplers and conclude. 

\section{Graphical Models (GMs)}
\label{sect:background}
Let $\vec{X} = \{X_1, X_2, \ldots\}$ be a set of random variables with realizations in the form 
$\vec{x} = \{x_1, x_2, \ldots\}$.\footnote{
As a common abuse of notation, in case there is no ambiguity, we do not distinguish between random variables and their realizations; e.g., we abbreviate $\pr(X_i = x_i)$ with $\pr(x_i)$.}
$\vec{X}$ may contain discrete and continuous variables. 
However, for the sake of notational consistency, %and since the problems tackled in this paper only hold for continuous variables, 
we only consider continuous models throughout. 
Note that inference in presence of discrete deterministic constraints is already addressed in the literature 
(see e.g.\ \cite{li2013dynamic} for related work).
Therefore, generalization of the presented framework to hybrid discrete/continuous models is straightforward.   

To cover both directed and undirected graphical models we use
\emph{factor graph} notation \cite{kschischang2001factor}
and represent a joint probability distribution $\pr(\vec{X})$ in a factorized form as follows: 
\begin{equation} \footnotesize
\label{e:factor-graph}
\pr(\vec{X}) \propto \prod_{\Psi_k \in \boldsymbol\Psi} \Psi_k (\vec{X}_k)
\end{equation}
where 
$\Psi(\vec{X}_k)$ are non-negative, real-valued \emph{potential functions} (of subsets $\bvec{X}_k$ of $\vec{X}$) and the normalization constant is not necessarily known.
%Throughout, we adopt the notation $\textsc{Scope}(\Psi_k) := \bvec{X}_k$.
%$C$ is a (not necessarily known) normalization constant.
%The inference procedure that will be presented does not require the normalization constant be known. 

\textbf{Inference.}
The inference task studied in this paper is 
the computation of the \emph{posterior} joint distribution 
$\pr(\bvec{Q} \,|\, \bvec{E}=\bvec{e})$
of 
a subset $\bvec{Q}$ (\emph{query}) of $\bvec{X}$ 
conditioned on (realization $\bvec{e}$ of) 
variables  
$\bvec{E} \subset\bvec{X} \backslash \bvec{Q}$ (\emph{evidence}):
\begin{equation}\vspace{-0.5mm}\footnotesize
\label{e:inference}
\pr(\vec{Q} \,|\, \bvec{E} =\vec{e}) \propto 
\int_{-\infty}^{\infty} \!\! \cdots \int_{-\infty}^{\infty}
%\prod_{\Psi \in \boldsymbol\Psi} \Psi(\vec{x})
\!\!\! \pr(\bvec{Q}, \bvec{W} = \bvec{w}, \bvec{E}=\bvec{e} )
 \dd \bvec{w}
\end{equation}
where $\bvec{W} = \{W_1, \ldots, W_m\} := \vec{X} \backslash (\vec{Q} \cup \vec{E})$.% and its realization is in form $\bvec{w} = \{w_1, \ldots, w_m\}$.

The main challenge for closed-form inference is the computation of multiple integrals required for \emph{marginalization} of $\bvec{W}$.
There are a few classes of functions which are closed under integration {\color{red}TODO ref?}.
Beyond these classes, for asymptotically unbiased reasoning MCMC sampling methods are often utilized {\color{red}TODO ref?}.  
%However, none of these classes can handle deterministic constraints beyond the level of linear constraints.

%%Computation of $N$ univariate integrals per sample is costly and this makes Gibbs sampling a relatively slow sampler. However, we will show that the family of PPF functions, the performance of Gibbs sampling can be improved significantly.
%In the next section, we formalize GMs with deterministic relations among random variables. 
%%%%%%%%%
\section{Stochastic-Deterministic Graphical Models (SD-GMs)}
\label{sect:contribution1}
In model specification that we are concerned with,
the network random variables $\bvec{X} := \bvec{Y} \cup \bvec{Z}$
where disjoint sets $\bvec{Y}$ and $\bvec{Z}$ are referred to as the sets of \emph{stochastic} and \emph{deterministic}  random variables respectively. 
The total joint probability (\ref{e:inference}) is factorized as:
\begin{equation}
\label{e:infer.sd}
\pr(\bvec{Y}, \bvec{Z}) \propto 
\prod_{i} \Psi_i (\bvec{Y}^s_i, \, \bvec{Z}^s_i)
\prod_j \delta \big(
Z_j - G_j(\bvec{Y}^d_j, \bvec{Z}^d_j)
\big)
\end{equation}
where \emph{stochastic potentials} $\Psi_i$ are functions 
of stochastic (and deterministic) variables 
$\bvec{Y}^s_i \subset \bvec{Y}$ 
(resp. $\bvec{Z}^s_i \subset \bvec{Z}$) that do not involve 
\emph{Dirac deltas} while 
each deterministic random variable $Z_j$ 
is associated with exactly one 
\emph{deterministic potential} in form 
$\delta\big( Z_j - G_j(\cdot) \big)$ 
 where 
%$\delta[\cdot]$ denotes \emph{Dirac delta} and 
$G_j$, the \emph{logical value} of $Z_i$,  is a function of 
stochastic variables $\bvec{Y}_j^d \subset \bvec{Y}$ and deterministic variables $\bvec{Z}_j^d \subset \bvec{Z}$. 
\footnote{
$Z_j \not \in \bvec{Z}_j^d$ 
and circular/recursive definition of deterministic variables is not allowed. }
%Note that models with DAG dependencies between deterministic nodes can be easily converted to the canonical form (\ref{e:infer.sd}).For instance in the collision model, potential $\delta[P_3 - (P_1 + P_2)]$ can be restated as $\delta[P_3 - (M_1 V_1 + M_2 V_2)]$ which is canonical.}).
%For example, a standard form for the Collision model is: $\Phi := \pr(M_1, M_2, V_1, V_2)$ (as in equation (\ref{e:col-prior})), and $\boldsymbol{\Psi^D} := \{\Psi_{P_1}, \Psi_{P_2}, \Psi_{P_3}\}$  where $\Psi_{P_1} := \delta[P_1 - M_1 V_1]$, $\Psi_{P_2} := \delta[P_2 - M_2 V_2]$ and $\Psi_{P_3} := \delta[P_3 - M_1 V_1 - M_2 V_2]$.

\subsection{Reduction of SD-GMs to purely stochastic GMs (Collapsing $\delta$)}
\label{sect:collapse}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\begin{algorithm}[tb]%[hb!]
\caption{{\sc PosteriorJoint}  
\label{alg:posterior-joint}}
\begin{algorithmic}
%{\small
\STATE {\bf Input: }{
$\mathbb{E} = \{\tuple{E_i, e_i}\}_i$ 
// \emph{\small evidence}\\
%$\bvec{Z} = \{Z\}$, deterministic variables\\
\qquad $\boldsymbol{\Psi} = \boldsymbol{\Psi^D} \cup \boldsymbol{\Psi^S}$ 
 //\emph{set of potentials.
 } }
\STATE {\bf Output} {posterior joint distribution of a subset of variables from which, all other variables can be decided.}
 \vspace{1mm}
%{
%{\bf Begin}//{
\STATE //\emph{{\bf \sc Step 1.} Instantiation of the observed variables:}
\FOR{{\bf all} $\tuple{E, e} \in \mathbb{E}$}
\STATE \textbf{for all } $\Psi \in \boldsymbol{\Psi}$ \textbf{ do } $\Psi \leftarrow \Psi|_{E \leftarrow e}$	
\ENDFOR %\textbf{end for}\\
\vspace{1mm}
//\emph{{\bf \sc Step 2.} Isolating deterministic variables:}
\FOR{{\bf all} $\Psi_Z = \delta [Z - G^Z] \in \boldsymbol{\Psi^D}$}
	\STATE \textbf{forall }{$\Psi \in \boldsymbol{\Psi}$} \textbf{ do } 
		{$\Psi \leftarrow \Psi|_{Z \leftarrow G^Z}$	}
\ENDFOR
 \vspace{1mm}
\STATE //\emph{{\bf \sc Step 3.} Joint factor formation:}
\STATE $\pr_{\bvec{e}} \leftarrow \prod_{\Psi \in \boldsymbol{\Psi^S}} \Psi$\\
     %$J \leftarrow 1$\\
	%\ForAll{$\Psi \in {\boldsymbol\Psi}^{S}$}
	%{
	%	$J \leftarrow J \otimes \Psi$
	%}
	%\textbf{end for}\\
 \vspace{1mm}
\STATE //\emph{{\bf \sc Step 4.} Collapsing determinism:}
\FOR {{\bf all} $\tuple{E, e} \in \mathbb{E}$ \textbf{such that} 
		$E \in \bvec{Z}$}
	\STATE \textbf{let} $\tuple{Y, G^Y} = \textsc{solve}(G^E=e)$ //  \emph{assuming that a unique solution exists.}
\STATE $\pr_{\bvec{e}} \leftarrow \pr_{\bvec{e}}|_{Y \leftarrow G^Y}$ 
	// \emph{substituting $Y$ with its solution}\\	
\ENDFOR
%	\textbf{end for}\\
\STATE {\bf Return} $\pr_{\bvec{e}}$
%}
%}%end small
%}
\end{algorithmic}
\end{algorithm}
%\decmargin{0.5em}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We allow both stochastic and deterministic variables to be observed.  
The key step of the algorithm is to handle observed deterministic random variables via reducing the dimensionality. 
That is, 
we construct a posterior joint distribution over a subset of stochastic random variables such that any other variable (either stochastic or deterministic)
can be expressed as a function of them:
%and therefore, effectively be sampled:
%The procedure
%, as formalized in Algorithm~\ref{alg:posterior-joint}, 
%is as follows: 

%\subsection{Posteriors in GMs with Deterministic Constraints}
%\label{sect:infer1}

\begin{enumerate}
\item \emph{Instantiation of observed variables.} 
In all potentials (either deterministic or statistical), all observed stochastic and deterministic random variables are instantiated
%\footnote{ A deterministic factor $\delta[Z_j- G_j(\cdot)]$ associated with an observed variable $Z_j = c$ is therefore substituted by $\delta[c- G_j(\cdot)]$.}
:
\begin{equation*}\footnotesize \vspace{-0.5mm}
\forall \Psi \in \boldsymbol{\Psi} \qquad \Psi \leftarrow \Psi|_{\bvec{E} \leftarrow \bvec{e}}
\end{equation*} 
\item 
\emph{Marginalizing determinism.} 
In all potentials, all deterministic variables $Z_j$, are 
substituted with their associated logical value $G_j(\cdot)$ (so, the potential $\delta ( Z_j - G_j(\cdot) )$ itself is replaced by 1.0). Note that by the definition of Dirac $\delta$, this simply means that  unobserved deterministic variables are marginalized out.
\begin{equation*}\footnotesize \vspace{-0.5mm}
\forall Z_j \in \bvec{Z},\,
\forall \Psi \in \boldsymbol{\Psi} 
\qquad
\Psi \leftarrow \Psi|_{Z_j \leftarrow G_j (\cdot)}
\end{equation*}
\item \emph{Joint factor formation.} the product of all stochastic potentials is computed: 
\begin{equation*}\footnotesize \vspace{-0.5mm}
\Phi := \prod_{i} \Psi_i
\end{equation*}
%
\item \emph{Dimension reduction.} %collapsing determinism 
%What remains is to condition on the deterministic constraints.
For each observed deterministic random variable $Z_j = c$ (where $c$ is a constant),
$(G_j(\cdot) - c)$ is solved w.r.t.\ some variable 
$Y  \in \bvec{Y}_j^d$ (assuming that it is solvable %with a single solution 
w.r.t.\ at least one variable with simple and real roots).
Let  
$\textsc{Solve}(G_j(\cdot) - c; \, Y)$
% := \{\Upsilon_{1}, \Upsilon_{2}, \ldots\}$ 
be the set of (real and simple) such solutions.
$\Phi$ is modified as follows:
\begin{equation}
\label{e:core}
\Phi 
\;
\longleftarrow
\!\!\!\!\!\!\!
\sum_{\Upsilon \in \textsc{Solve}(G_j(\cdot) - c;\, Y)}
\frac{\Phi|_{Y \leftarrow \Upsilon}}{
\big|(\partial G_j(\cdot) / \partial Y) |_{Y \leftarrow \Upsilon}
\big|
}
\end{equation}
\end{enumerate}
The end result $\Phi$ is proportional to 
$\pr(\bvec{Y}\backslash \bar{\bvec{Y}} \,|\, \bvec{E} = \bvec{e})$ in which 
$\bar{\bvec{Y}}$ is the set of stochastic variables eliminated in
step~4.
%{\color{red}
%As required, the eliminated variables $\bar{\bvec{Y}}$ and $\bvec{Z}$ are deterministically decided given  $\bvec{Y}\backslash \bar{\bvec{Y}}$.} 

For instance in the Collision model, 
the potentials can be indexed as follows: 
$\Psi_1$ to $\Psi_4$ as in (\ref{e:collision}),
$\Psi_5 := \delta(P_1 - M_1V_1)$,
$\Psi_6 := \delta(P_2 - M_2V_2)$ and
$\Psi_7 := \delta(P_3 - (P_1 + P_2))$.   
For evidence $\bvec{E} = \{(P_3 = 3.0)\}$, the algorithm runs as follows:
In Step 1, $\Psi_{7}$ is converted to $\delta(3.0 - (P_1 + P_2))$. %while the other potentials are not modified.
In Step 2, $\Psi_5$ is converted to $\delta(M_1 V_1 - M_1 V_1)$
(which is equal to $1$). For the similar reason $\Psi_6$ is converted to $1$ and $\Psi_7$ to $\delta(3.0 - (M_1 V_1 + M_2 V_2))$.  
In Step 3, $\Phi$, the product of $\Psi_1$ to $\Psi_4$ is computed. In the absence of observed stochastic variables, this simply equals the prior joint: 
$\Phi = \Pr(M_1, M_2, V_1, V_2)$ (equation~ \ref{e:col-prior}).
In step 4, $(G_7 - 3.0)$, i.e.\ $((M_1 V_1 + M_2 V_2) - 3.0)$, is solved w.r.t.\ 
 $M_1$ (It could alternatively be solved w.r.t. $V_1$, $M_2$ or $V_2$)
which leads to a singleton set of solutions:
{\footnotesize
$\textsc{Solve}(3.0 - M_1 V_1 - M_2 V_2; M_1) = \{\frac{3 - M_2 V_2}{V_1}\}$
}.
Finally, since  
$\left| \frac{\partial (M_1 V_1 + M_2 V_2)}{\partial M_1} \right| = |V_1|$, by (\ref{e:core}),
$\Phi$ is converted to:  
%%
{\footnotesize
\begin{equation}  
\label{e:col-prior2}
%\pr(M_2, V_1, V_2)  =
\begin{cases}
\frac{1}{V_1(16 V_1 + 32)} &{\text{if }\scriptstyle 0<V_1, \, 0.1<\frac{3-M_2 V_2}{V_1}<2.1, \, 0.1<M_2<2.1,}\\
							 &{\;\;\, \scriptstyle -2<V_1<2, \, -2<V_2 < V_1}\\
\frac{-1}{V_1(16 V_1 + 32)} &{\text{if }\scriptstyle V_1<0, \, 0.1<\frac{3-M_2 V_2}{V_1}<2.1, \, 0.1<M_2<2.1,}\\
							 &{\;\;\, \scriptstyle -2<V_1<2, \, -2<V_2 < V_1}\\
 \otherwise{0}
 \end{cases}
\end{equation}
}
which is proportional to $\pr(M_2, V_1, V_2 \,|\, P_3 = 3)$.

Note that dimension reduction (step 4) is 
in fact consecutive applications of  Theorem~\ref{theorem1}.

\begin{theorem} 
\label{theorem1}
Let {\footnotesize $\pr(Z\!=\!z | x_1, \ldots, x_n) = \delta[f(x_1, \ldots, x_n)-z]$}, 
where $f(x_1, \ldots, x_n) = 0$ has real and simple roots for $x_1$ with a non-vanishing continuous derivative
$\partial f(x_1, \ldots, x_n) / \partial x_1$ at all those roots.
Denote the set of all roots by 
 %let the set of all real and simple roots of the deterministic constraints with respect to $x_1$ be
 $ \mathcal{X}_1 = \{ x_1 \; | \; f(x_1, \ldots, x_n) - z = 0 \} $. 
(Note that each element of $ \mathcal{X}_1 $
 is a function of the remaining variables $ x_2,\dots,x_n,z $.)
 Then:
\begin{multline}
\label{e:theorem1}
p(x_2, \ldots, x_n \,|\, Z=z) \propto \\
\sum_{x_1^i \in \mathcal{X}_1} 
\frac{p(X_1=x_1^i, x_2, \ldots, x_n)}
{\Big|\big(\partial f(x_1, \ldots, x_n) / \partial x_1 \big)|_{x_1 \leftarrow x_1^i} \Big|}
\end{multline}
\end{theorem}
\begin{proof}
$p(x_2, \ldots, x_n \,|\, Z=z)$ is proportional to
\begin{align}
%p(x_2, \ldots, x_n \,|\, Z=z) = \\
%
%\int_{x_1=-\infty}^{\infty}p(x_1, \ldots, x_n \,|\, Z=z) dx_1 \propto \\
%
%&\int_{x_1=-\infty}^{\infty}p(x_1, \ldots, x_n, Z=z) dx_1 =\\
%
&\int_{-\infty}^{\infty}p(x_1, \ldots, x_n)p(Z=z \,|\, x_1, \ldots, x_n) \dd x_1 \notag\\
%
=&\int_{-\infty}^{\infty}p(x_1, \ldots, x_n)
\delta \big( f(x_1, \ldots, x_n) - z \big) \dd x_1 
\label{e:fand}
\end{align}
According to \cite{gel1964generalized}
there is a unique way to define the composition of Dirac delta with 
an arbitrary function $h(x)$:
\begin{equation}
\label{e:gelfand}
\delta(h(x)) = \sum_{i} \frac{\delta(x - r_i)}{|\partial h(x)/\partial x|}
\end{equation}
where $r_i$ are all (real and simple) roots of $h(x)$ and $h(x)$ is continuous and differentiable in the the root points. By (\ref{e:gelfand}), (\ref{e:fand}) and 
\emph{Tonelli's theorem}\footnote{Tonelli's theorem says that for non-negative functions, sum and integral are interchangeable.} 
$\pr(x_2, \ldots, x_n \,|\, Z = z) \propto$
\begin{equation*}
\sum_{x_1^i \in \mathcal{X}_1} 
\frac{\int_{-\infty}^{\infty} p(x_1, x_2, \ldots, x_n)  \delta(x_1 - x_1^i) \dd x_1}
{\Big|\big(\partial f(x_1, \ldots, x_n) / \partial x_1 \big)|_{x_1 \leftarrow x_1^i} \Big|}
\end{equation*}
which implies (\ref{e:theorem1}).
\end{proof}
%
%Note that elimination of Dirac deltas requires transformations  that often leads to formulas without known closed-form integrals. This makes closed form inference (by Eq.~\ref{e:inference}) impossible. Nonetheless, inference can still be done via sampling.
Given the end result ${\Phi}$, inference is carried out by means of sampling.

Theorem~\ref{theorem1} tackles the core problem of delta elimination in a general setting; however, 
for simplicity, we restrict the scope of the discussion to cases where  
In step 4, \ $G_j(\cdot) - c$ is invertible w.r.t. at least one variable $Y$ 
i.e.\ $\textsc{Solve}(G_j(\cdot)-c)$ is singleton (\emph{invertibility assumption}).{
Invertibility assumption holds
if $G_j(\cdot)$, the logical values of $Z_j$, are linear w.r.t.\ some variables. 
That is, for some variable $Y$, $G_j(\cdot)$ can be stated as: 
{\footnotesize
$$
%\label{eq:evidence-form}
Gj(\cdot) = 
{(\mathcal{A} \cdot Y + \mathcal{B})}/{(\mathcal{C} \cdot Y + \mathcal{D})}
$$}
\!\!where $\mathcal{A}$ to $\mathcal{D}$ are polynomials and $Y$ is not in their domain.

%{\color{red}(unique solution assumption(?))}.
%The reason is that in this case,  the samples of the eliminated variables $\bar{\bvec{Y}}$ and $\bvec{Z}$ are deterministically decided given the samples of $\bvec{Y}\backslash \bar{\bvec{Y}}$ while in the general setting, more operations may be required.
%Given the samples for $\bvec{Y} \backslash \bar{\bvec{Y}}$, samples of deterministic variables $\bvec{Z}$ can be su
%

%It is worth mentioning that while Theorem~\ref{theorem1} tackles the core problem of delta elimination in the general setting, with the invertibility assumption restriction, the theorem can alternatively be proved via transformation of variables: deriving $\pr(z, x_2, \ldots. x_n)$ from $\pr(x_1, \ldots, x_n)$ followed by utilizing the \emph{inverse function theorem}.  

%We are interested in observed deterministic variables with logical values  
%created by elementary arithmetic operators ($+$, $-$, $\times$, $\div$) and therefore 
%in the form of fractions of polynomials.




{\color{red}
The last subtle point in conditioning on determinism is that apparently equivalent deterministic observations may lead to different posteriors. For example evidence $(E_1 = 0)$ where $\pr(E_1)= \delta(E_1 - M V + 3)$ and 
evidence $(E_2 = 0)$ where $\pr(E_2) = \delta(E_2 - M + \frac{3}{V})$ lead to different posteriors. 
Although both may seem to formalize the same constraint $(MV = 3)$.

It should be pointed out that the same issue arises when hard constraints are approximated by adding noise:
$\mathcal{N}(E - MV + 3, \sigma) \neq \mathcal{N}(E -M + \frac{3}{V}, \sigma)$).
The correct approximation form depends on the nature of observation.
%The reason can be understood by considering 
Dirac deltas can be understood as the limit of normal distributions with $\sigma \rightarrow 0$.
Therefore, they inherit the same issue: 
The canonical way to express a deterministic relationship is a form to which one would add noise if one had to approximate the determinism which is model-dependent. 
  
%A subtle point in conditioning on deterministic relationships is that the resulting density is affected by the value of the prior in the limit of the ambient space surrounding the posterior hypersurface (or submanifold). Therefore, an observed deterministic relationship  (e.g.\ $M_1 V_1 = 3$) cannot be represented by equivalent forms (e.g.\ $M_1 - \frac{3}{V_1} = 0$) in the same way the noisy estimation of the former and latter produce different distribution (e.g.\ $N((M_1 V_1 - 3), \sigma)$ differs $N((M_1 - \frac{3}{V_1}), \sigma)$).
}

\section{Polynomial Piecewise Fractional Functions (PPFs)}
\label{sect:ppfs}
%In this section, we put things together and provide our complete model.
%The requirements for the class of potential functions (factor forms) are as follows:
%\begin{itemize}
%\item Factor forms should be as expressive as possible to be able simulate different density forms.  
%\item Factor forms should be closed under multiplication and polynomial fractional substitutions such as the r.h.s.\ of equation~(\ref{eq:evidence-form}). Otherwise, \emph{joint factor formation} and multiple substitutions in the \emph{collapsing determinism} phase of Algorithm~\ref{alg:posterior-joint} can be problematic.
%\item A single analytical integration 
%(i.e.\ integrating w.r.t.\ a single variable while keeping the rest of variables symbolic) should produce a closed-form structure (the requirement of Symbolic Gibbs).
%\end{itemize}
%Our proposition is the use of  the class of \emph{Polynomial-Piecewise Polynomial Fractions} (PPFs); That is, functions in the form:   
We study piecewise fractional functions with polynomial partitioning conditions (PPFs) -- a highly expressive class of functions neglected in the literature so far.
That is, functions in form:
\begin{equation}\footnotesize
\label{e:ppf}
f = 
  \begin{cases}
  \case{\phi_1}{f_1}\\
\vdots\\
  \case{\phi_m}{f_m}    
  \end{cases}
\!\!\!=
  \begin{cases}
  \case{\varphi_{1,1} \lessgtr 0,\, \varphi_{1,2} \lessgtr 0,\, \ldots}{\frac{N_1}{D_1}} \\
\vdots\\
   \case{\varphi_{m,1} \lessgtr 0,\, \varphi_{m,2} \lessgtr 0,\, \ldots}{\frac{N_m}{D_m}}    
  \end{cases}
\end{equation}
where each \emph{sub-function} $f_i := \frac{N_i}{D_i}$ is a (multivariate) polynomial fraction and
\emph{conditions} $\phi_i$ partition the space of function variables. 
\emph{conditions} $\phi_i$ are conjunction of some inequalities ($\lessgtr$ stands for  
$>$ or $<$)\footnote{In piecewise distributions,  
we are careless about equalities since they only matter for Dirac deltas which are treated separately.
%We assume that no $\delta[h(\bvec{x})]$ has roots point on the partitioning hyperplanes.
} 
where each \emph{atomic constraint} $\varphi_{i,j}$ is a polynomial.

%Evidently, this class is very expressive. Other distributions can also be approximated by this form by being \emph{Taylor series} other existing approximation tools \cite{shenoy2011inference}.

PPFs are closed under elementary operations, e.g.:
\begin{equation*}
\footnotesize
  \begin{cases}
  \case{\phi_1}{f_1}\\
  \case{\phi_2}{f_2}    
  \end{cases}
\,
 \times
\,
  \begin{cases}
  \case{\psi_1}{g_1} \\
  \case{\psi_n}{g_2} 
  \end{cases}
 \, = \,
\begin{cases}
  \case{\phi_1, \psi_1}{f_1 \times g_1} \\ 
  \case{\phi_1, \psi_2}{f_1 \times g_2} \\
  \case{\phi_2, \psi_1}{f_2 \times g_1} \\
  \case{\phi_2, \psi_2}{f_2 \times g_2}
  \end{cases}
\end{equation*} 
Their closure under fractional substitution follows from the fact that 
fractional conditions can be restated as polynomials:
\begin{align*}
%\label{e:fractional2nlpa}
\left(
 \begin{cases}
  \case{\frac{F}{G} > 0}{f_1} \\ 
 \end{cases} 
\right)
 =
{\footnotesize
\begin{cases}
  \case{F > 0, G > 0 }{f_1} \\ 
  \case{F \leq 0, G \leq 0}{f_1} \\ 
 \end{cases} 
}
\end{align*}
It follows that PPFs are also closed under \emph{absolute value}.
Therefore, if the solutions to $G_j(\cdot)$ are fractional (as it is the case under invertibility assumption),
PPFs are closed under  observed algebraic fractional determinism 
(since they are closed under  
all operations required for collapsing determinism).
%This is an important characteristic that other distribution families lack.  

%Note that PPFs are not closed under integrations
%required for closed-form inference (see Equation~\ref{e:inference}).

%However, 
In general, PPFs are not closed under integration; however, a large subset of them 
 have closed-form single variable integrals.
As it will be seen in the next section,
this leads to an extremely efficient Gibbs sampling mechanism.   
%We will show that this is also holds for a large group of PPFs.

For brevity, we only focus on the following fairly expressive  subset of PPFs:

\textbf{PPF*. }
A PPF* is a PPF in which %s as in Definition~(\ref{e:ppf}) where 
each $\varphi_{i,j}$
can be written as a product of some terms $t_{i,j,k}$ where the maximum degree of each variable $X \in \bvec{X}$ in each $t_{i,j,k}$ is less or equal to 4 % (treating other variables as constants).%($\text{rel-degree}(t_{i,j,k}) \leq 4$)
and the sub-function denominators, $D_{i}$, can be factorized into polynomials ${D}_{i,h}$ where the maximum degree of each variable in each $D_{i,h}$ is less or equal to 2.% ($\text{rel-degree}(D_{i, k}) \leq 2$).%, where function $\text{rel-degree}(\cdot)$, the relative degree, is defined as follows:
 %\begin{enumerate} 
%\item Each constrain $\phi_i := \varphi_{i,1} \wedge \varphi_{i,2} \wedge \ldots$ is a conjunction of polynomial (in)equalities $\varphi_{i,j}$ such that $\text{rel-degree}(\varphi_{i,j}) \leq 4$.
%\item Each sub-function $f_i  := \frac{{N}_{i}}{{D}_{i,1} \cdot {D}_{i,2} \cdots}$
%is in the form of a fraction where the numerator ${N}_i$ 
%is an arbitrary polynomial and the denominator is factorized.
%\end{enumerate}
%\textbf{Relative degree of polynomials.} $x\text{-deg}(g)$, the degree of a polynomial $g$ w.r.t.\ a variable $x$, is the degree of $g$ if all scope variables except $x$ are considered constant. For example $x\text{-deg}(x y^2 + x^2 y z^3) = 2$. The \emph{relative degree} of $g$, $\text{rel-degree}(g)$, is:
%\[\text{rel-degree}(g) = \max_{x \in \vec{x}}(x\text{-deg}(g))\]

Here is an example of a PPF* case statement:
\begin{equation*}\vspace{-0.5mm}
{\footnotesize
\singlecase{y^2-x\leq 0 \wedge x^3+2xy \geq 0}{\frac{x^2 y^3 + 7x + 10}{(5x.y^2 + 2)(y + x)^5}}
}
\end{equation*}

%The class of PPF*'s is still quite expressive. Meanwhile PPF*'s have closed form (single) integrals that are not very hard to compute automatically. 

\subsection{Analytic integration of PPF*s}
The space limitations do not allow us to explain the integration process in detail. 
Instead, we highlight the main issues.

Firstly, note that the definite integrals can be reduced to indefinite integrals:
{
\footnotesize\vspace{-0.5mm}
\begin{align*}
\int_{\alpha}^{\beta} f \dd x  = 
\int_{-\infty}^{\infty}
\left (
  \begin{cases}
  \case{x\!>\!\alpha,\, x\!<\!\beta,\, \alpha \!<\! \beta}{1}\\
 \otherwise{0}    
  \end{cases}
\otimes
  f
\right)
\dd x
\end{align*}
}
Secondly, note that a PPF* can be restated into a form where
the degree of each variable in each atomic constraint is less or equal to 4.
E.g. consider a single case statement with a single atomic constraint 
$\varphi_{1,1} := t_{1,1,1} \cdot t_{1,1,2}$:% (where the degree of each variable in either $(t_{1,1,k})\leq4$):
\begin{align*}
{\footnotesize\vspace{-0.5mm}
\left(
f_1 \quad \text{if } t_{1,1,1} \cdot t_{1,1,2} > 0
\right)
 =
\begin{cases}
  \case{t_{1,1,1} \!>\! 0, \, t_{1,1,2} \!>\! 0 }{f_1} \\ 
  \case{t_{1,1,1} \!\leq\! 0, \, t_{1,1,2} \!\leq\! 0 }{f_1} 
 \end{cases} 
}
\end{align*}
Thirdly, note that for each variable $x$, 
PPFs can be transformed into piecewise structures 
in which each atomic constraint is either in form $x>L_i$ or $x<U_i$ or $I_i>0$, 
where $L_i$, $U_i$ and $I_i$ are 
algebraic expressions (not necessarily polynomials) and $x$ is not in their scope. 
%\footnote{Again, we are careless about equalities.} 
%For instance a case statement:
%\begin{align*}
%{\footnotesize
% \begin{cases}
%  \case{(xy + z) > 0 \wedge \cdots}{f_1} \\ 
%  \vdots\\
% \end{cases} \!\!\!\!\!\!=
%\begin{cases}
%  \case{(y >0) \wedge (x>\frac{z}{y}) \wedge \cdots}{f_1} \\ 
%  \case{(y <0) \wedge (x<\frac{z}{y}) \wedge \cdots}{f_1} \\ 
 % \vdots\\
% \end{cases}
%}
%\end{align*}
%Note that in the case of quadratic constraints, these transformed cases are not necessarily polynomials.
For instance for variable $x$, the case-statement in (\ref{e:single-example})
can be replaced with three cases as in (\ref{e:quadratic-trans}).
{
\footnotesize
\begin{equation}
\label{e:single-example}
\singlecase{(x^2 y + 7x^2 + x + y > 0)}{f_1}
\end{equation}
\begin{align}
\label{e:quadratic-trans}
{
\begin{cases}
  \case{(y+7>0), \, (x> -0.5 + 0.5\sqrt{1 - 4y(y+7)}) }{f_1} \\ 
  \case{(y+7>0), \, (x> -0.5 - 0.5\sqrt{1 - 4y(y+7)}) }{f_1} \\ 
  \case{(y+7<0), \, (x > -0.5 + 0.5\sqrt{1 - 4y(y+7)}),\\
& (x < -0.5 - 0.5\sqrt{1 - 4y(y+7)}) }{f_1} \\ 
 \end{cases}
}
\end{align}
}
The infinite bound integral of a case-statement 
associated with expressions with $\{L_i\}$, $\{U_i\}$ and $\{I_i\}$ 
is itself a case-statement with the same independent constraints,
a lower bound LB =$\max\{L_i\}$ and 
an upper bound UB =$ \min\{U_i\}$.
For example:
{\footnotesize 
\begin{align*}
%\footnotesize
&\int_{-\infty}^{\infty}\!\! \Big[
\singlecase{(x>3), (x>y+1), (x<y^2-7), \\
&\hspace{28mm} (-y/z > 1) , (y^2>0)}
{x^3 + xy} \Big] \dd x = \\
&\singlecase{(-y/z > 1) \wedge (y^2>0)}
{\Big[ \int_{\max\{3, \, y+1\}}^{y^2 - 7} (x^3 + xy) \dd x \Big]} 
\end{align*}  
}
What remains is to compute the indefinite integrals of sub-functions. 
The restrictions imposed on PPF* sub-functions 
guarantee that they have closed form indefinite univariate integrals.
These integrals are computed by performing polynomial division (in case needed),
followed by partial fraction decomposition and finally, using a short list of indefinite integration rules.

%The output of this algorithm is a non normalized joint distribution of  a subset of none-deterministic variables. Provided with this joint, the marginal distribution of all query variables (either probabilistic or deterministic) can be computed by means of sampling as will be discussed later.

%Note that these operations alter the normalization constant and the resulting function is only proportional to the corresponding joint distribution. However, most inference algorithms do not require normalization.Some of these algorithms are provided in the next section. However, prior to that we discuss the limitations of Algorithm~\ref{alg:posterior-joint} 

%The family of deterministic constraints studied in this work includes polynomial fractions. For this family, the unique solution assumption (required in Step 4 of Algorithm~\ref{alg:posterior-joint}) in general holds if $G^Z$, the logical values of deterministic variables, are linear w.r.t. some variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Symbolic Gibbs Sampling}
\label{sect:symbolic.gibbs}

Gibbs sampling is a robust MCMC tool in the sense that it does not require known bounds or any kind of tuning.
In Gibbs, drawing a sample for $\bvec{X} := (X_1, \ldots, X_N)$ takes place in $N$ steps.
In the $i$-th step, $X_i$ is sampled conditioned on the current realization of the others variables:
$x_i \sim \pr(X_i \,|\, \bvec{x}_{-i})$
\cite{bishop2006pattern}.
To do so, the univariate (conditional) 
\emph{cumulative distribution function} (CDF)
is computed by (\ref{e:cdf.0}) and a sample is taken by \emph{inverse transform sampling}. 
\begin{equation} \footnotesize
\label{e:cdf.0}
\text{CDF}(X_i  \,|\, \bvec{x}_{-i}) 
\propto
\int_{-\infty}^{X_i} \!\! \pr(X_i = t, \,\bvec{X}_{-i} = \bvec{x}_{-i})  \dd  t
\end{equation} 
%
Our \emph{symbolic Gibbs sampling} is based on a simple but significantly useful insight:
If $\pr(\bvec{X})$
has analytic integrals w.r.t.\ all variables $X_i$,
then the costly CDF computations can be done prior to the sampling process rather than $N$ times per sample. 
It is sufficient to construct a map (called $\textsc{VarToCDF}$) from variables $X_i$ to their corresponding (unnormalized conditional) analytical CDF. 
%then, instead of computing $N$ univariate integrals (required in Eq.~\ref{e:cdf}) per sample, we can perform the costly computations prior to the actual sampling process by constructing a map from each variable $X_i$ to its corresponding univariate symbolic CDF: 
\begin{equation} \footnotesize
\label{e:cdf-symbolic.0}	
%F(X_i  \,|\, \bvec{X}_{-i}) 
\textsc{VarToCDF}\text{.put}
\left(
X_i, \, % \;\; \longrightarrow \;\;
%\propto
\int_{-\infty}^{X_i} \!\!\!\!\! \pr(X_i = t , \bvec{X}_{-i}) \dd  t
\right)
\end{equation} 
Note that the difference between (\ref{e:cdf.0}) and (\ref{e:cdf-symbolic.0}) is that in the former, 
all variables except $X_i$ are already instantiated therefore 
$\text{CDF}(X_i  \,|\, \bvec{x}_{-i})$ is a univariate function but in  (\ref{e:cdf-symbolic.0}), 
although variables $\bvec{X}_{-i}$ are treated as constants but they are kept uninstantiated and symbolic.
%As a result, $F(X_i \,|\, \bvec{X}_{-i})$ is a multivariate analytic function. 
%as Algorithm~\ref{alg:analytic-cdf} shows, 
%we can create a map from variables $X_i$ to their corresponding closed-form CDFs.

Provided with such a map, in the actual sampling process, 
to sample $x_i \sim \pr(X_i \,|\, \bvec{x}_{-i})$,
it is sufficient to instantiate the analytical CDF associated to $X_i$ with  
$\bvec{x}_{-i}$ to obtain the appropriate univariate conditional CDF
(see Algorithm~\ref{alg:symbolic-gibbs}).

Note that in theory, CDF inversion (required for inverse transform sampling)
can also be computed analytically leading to a totally analytical Gibbs sampler.
However, analytical CDF$^{-1}$ functions can be quite sophisticated and hard to compute. 
In our current implementation, CDF inversion is performed via \emph{binary search}. This requires several function evaluations (i.e.\ instantiations) per sample. Nonetheless, (unlike integration) function evaluation is a very fast operation. Therefore, further optimization may not be justified.    

%By this method, instead of computing $N \times T$ integrals for $T$ samples, only $N$ integrals are computed and $N \times T$ function instantiations are performed. But function instantiation is much faster than integration. Therefore, the algorithm leads to a major improvement in speed.

%What remains is to show that a large subset of PPF functions indeed have closed-form integrals. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[hb!]%[tb]
\caption{{\sc SymbolicGibbs}  
\label{alg:symbolic-gibbs}}
\begin{algorithmic}
 \STATE {\bfseries Input:}
{$\bvec{X} := \tuple{X_1, \ldots, X_N}$, 				\hspace*{\fill}// \emph{\small random variables} \\
 $\bvec{x}^{(0)} := \tuple{x_1^{(0)}, \ldots, x_N^{(0)}}$, 	\hspace*{\fill}// \emph{\small initial values} \\
{\sc VarToCDF}, 								\hspace*{\fill}//\emph{\small variables to analytic CDFs map} \\
 $T$ 										\hspace*{\fill}// \emph{\small number of desired samples } }
\STATE{\bf Output:}{ list of taken samples}
//%\BlankLine
{\small
%{\bf Begin}//  
\FOR{$t= 1, 2, \ldots T$}
	 \STATE $\tuple{x_1^{(t)}, \ldots, x_N^{(t)}} \leftarrow \tuple{x_1^{(t-1)}, \ldots, x_N^{(t-1)}}$    
	\FOR{ {\bf each} $X_i \in \bvec{X}$}
		\STATE $F(\bvec{X}) \leftarrow \textsc{VarToCDF}.\text{get}(X_i)$ //\emph{analytic CDF w.r.t.\ $X_i$:}
		\STATE //\emph{univariate CDF w.r.t.\ $X_i$:}
		\STATE 	$G(X_i) \leftarrow F(x_1^{(t)}, \ldots, x_{i-1}^{(t)}, X_i, x_{i+1}^{(t)}, \ldots, x_N^{(t)})$ 
		\STATE //\emph{$G(\infty)$ is the normalization constant}
		\STATE $x_i^{(t)} \leftarrow \textsc{InverseTransformSample}(\frac{G(X_i)}{G(\infty)})$
	\ENDFOR %end for each x_i
\ENDFOR %end for t
\STATE {\bf Return} {$\big\langle
			\tuple{x_1^{(1)}, \ldots, x_N^{(1)}}, \ldots, 
			\tuple{x_1^{(T)}, \ldots, x_N^{(T)}}
		\big\rangle$}\;
%	ForEach{}{
%	
%	}
%	\textbf{end for}\\
 %  }
   %\textbf{end for}\\   
%}
} %end small
\end{algorithmic}
\end{algorithm}


%FIG BLUE1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\vspace{-1mm}
\begin{center}
\begin{tabular}{cccccc}
   \hspace{-5mm} \includegraphics[width=\nn\textwidth]{Figs/col_m1_v1.png} 
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs/col_m1v1_p3.png} 
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs/col_m1v1_when_p_is_3_and_v2_is_0_dot_2.png}
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs/col_v1v2.png}
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs/col_v1v2whenPis3.png}
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs/colV1V2givenPis3M1is2.png}
\vspace{-1.5mm}
\\
   \hspace{-5mm} \footnotesize(a) 
& \hspace{-4mm} \footnotesize(b) 
& \hspace{-3mm} \footnotesize(c) 
&\hspace{-1mm} \footnotesize(d) 
&\hspace{-1mm} \footnotesize(e) 
&\hspace{-1mm} \footnotesize(f)\\
\multicolumn{6}{c}{}
\end{tabular}
\end{center}
\vspace{-8mm}
\caption{\footnotesize
Prior/posterior joint distributions of pairs of random variables in the \emph{collision} example. 
(a) $\pr(M_1, V_1)$,
(b) $\pr(M_1, V_1 \, | \, P_3 = 3)$,
(c) $\pr(M_1, V_1 \, | \, P_3 = 3, V_2 = 0.2)$,
(d) $\pr(V_1, V_2)$,
(e) $\pr(V_1, V_2 \, | \, P_3 = 3)$,
(f) $\pr(V_1, V_2 \, | \, M_1 =2, P_3 = 3)$
using rejection sampling on the $\delta$-collapsed model.
} 
\label{fig:mom}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%FIG BLUE 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\vspace{-1mm}
\begin{center}
\begin{tabular}{cccccc}
   \hspace{-5mm} \includegraphics[width=\nn\textwidth]{Figs2/col_c_gibbs10000.png} 
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs2/col-c-mh-10000_08.png} 
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs2/col_c_stan10000_02.png}
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs2/col_c_stan_10000_000001.png}
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs2/col_c_ang10000_02_001.png}
& \hspace{-3mm} \includegraphics[width=\nn\textwidth]{Figs2/col_c_ang_10000_01_001}
\vspace{-1.5mm}
\\
   \hspace{-5mm} \footnotesize(a) 
& \hspace{-4mm} \footnotesize(b) 
& \hspace{-3mm} \footnotesize(c) 
&\hspace{-1mm} \footnotesize(d) 
&\hspace{-1mm} \footnotesize(e) 
&\hspace{-1mm} \footnotesize(f)\\
\multicolumn{6}{c}{}
\end{tabular}
\end{center}
\vspace{-8mm}
\caption{\footnotesize
10000 samples taken from the distribution Fig.~(\ref{fig:mom}-c)
using (a) Baseline Gibbs sampler and(b) MH with \emph{proposal variance} 0.8 
on the reduced-dimension model as well as  
(c) Hamiltonian Monte Carlo (HMC) with a measurement error variance 0.2, 
, (d) and 0.01 as well as Anglican implementation of SMC alg. with %RDB alg. with 
parameters (e)
%$V_2$ and $P_3$ observation noise variance 
$\sigma^2_{V_2} = 0.01$, $\sigma^2_{P_3} = 0.2$ and 
(f) $\sigma^2_{V_2} = 0.01$, $\sigma^2_{P_3} = 0.1$
on the \emph{approximated-by-noise} model.
}
\label{fig:mom2}
\vspace{-4mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}
\label{sect:experimental.results}
Our experiments are to study
(a) the characteristics of the posterior joints, 
(b) the mixing rate of MCMC methods and
(c) the effect of approximation of hard constraints
in models with deterministic conditionals. 

%Our experiments are to address the following questions: 
%\\ (a) Conditioned on hard constraints, how do posteriors look like?
%\\ (b) In terms of convergence to ground truth, how do MCMC methods perform on such models? 
%\\ (c) How does approximation of deterministic conditionals via adding noise to the observation affects the sampling convergence rate? 

\textbf{Experiment 1.} We sample posteriors associated with our running example (\emph{collision model}) for different evidence configurations.
In each setting, 10000 samples are generated by \emph{rejection sampling} (Rej).
%and plot the particles for different queries and evidence.%\footnote{We have repeated the experiment for different MCMC methods and have made sure they generate same patterns.}
The results, as depicted in the telltale plots of Figure~\ref{fig:mom}, reveal the nature of the problem tackled in this paper.
They show that even in the presence of a single deterministic constraint, 
a simple prior can be transformed into a sophisticated piecewise and multimodal posterior that does not resemble any family of 
probability densities studied in the literature so far. 
%It can also be seen that more evidence generally lead to more complicated posteriors.
%The reason is that more evidence lead to more difference between the dimensionality of the non-transformed posterior submanifold and the dimensionality of the random variable space increases.
%The true posteriors are approximated by 10000 particles generated by rejection sampling -- an unbiased mechanism that is functional in low dimensions as is the case of this experiment. 

In Figure~\ref{fig:mom2}, the experiment is repeated on the posterior of 
Fig.~\ref{fig:mom}-c using different MCMCs:
The following algorithms are run on the \emph{$\delta$-collapsed} distribution 
made by the algorithm of Section~\ref{sect:collapse}:
\\
%
I.\ Baseline Gibbs (Gibbs) (samples depicted in Figure~\ref{fig:mom2}-a), 
II.\ Symbolic Gibbs (SymGibbs) (not depicted since they were not visually distinguishable from the output of (Rej) of (Gibbs))
and
III.\ Metropolis-Hastings (MH) automatically tuned  
after \cite{roberts1997weak})\footnote{
\label{foot:tuning}
Among 200 equidistant proposal variances in interval 
$(0, 0.1]$ %are tested by taking 100 samples using each and  
the one with the acceptance rate closer to 0.24 is chosen.
} ( Fig.~\ref{fig:mom2}-b).
Although the same 10000 samples is generated by all algorithms, (MH) looks sparse 
(see Fig.~\ref{fig:mom2}-b) 
due to its low \emph{effective sample size} (repeated samples).

The remained algorithms are the-state-of-the-art MCMCs running on the model with \emph{determinism approximated by Gaussian noise}:
IV. Hamiltonian MCMC (HMC) using Stan framework \cite{stan-manual:2014} 
with added noise variances 0.2 (Figure~\ref{fig:mom2}-c)
and 0.01 (Figure~\ref{fig:mom2}-d).\footnote{
Comparing Figures \ref{fig:mom2}-c \& d shows that HMC is not very sensitive to noise parameter tuning. This might be due to Stan's internal parameter manipulation (?)
}
V.  Anglican implementation of \emph{Sequential
Monte Carlo (SMC)} \cite{wood2014new},
with noise parameters $\sigma^2_{V_2} = 0.01 \& \sigma^2_{P_3} = 0.2$ (Figure~\ref{fig:mom2}-e)
as well as $\sigma^2_{V_2} = 0.01 \& \sigma^2_{P_3} = 0.1$ (Figure~\ref{fig:mom2}-f).\footnote{
Anglican syntax promotes adding noise to all observed variables regardless of being deterministic 
(here $P_3$) or stochastic (here $V_2$). 
 }

%and adding noise to multiple variables deteriorates the performance.
 

In Figure~\ref{fig:asymmetric} 
 
%the \emph{collapsed-$\delta$} distribution made by algorithm (Section~\ref{sect:collapse})
%Gibbs sampling on the {\color{green} reduced-dim} model generates results that are not distinguishable from the output of rejection sampling
%(Figure~\ref{fig:mom2}-a).10000 samples generated by MH on the same model looks sparse ( Fig.~\ref{fig:mom2}-b) since many sample points are returned multiple times leading to low \emph{effective sample size}.

%In the remained figures, samplers are taken via adding normal noise to the determinism rather than reducing the dimensionality.
%Figures~\ref{fig:mom2}-c, \ref{fig:mom2}-d correspond to Hamiltonian MCMC (using Stan framework) with noise variances 0.2 and 0.01 respectively. Despite the high difference between these parameter values, the end results are not significantly different. 
%This may be due to Stan internal noise parameter discretization to prevent near-deterministic MCMC issues. 
%Finally, in Figures~\ref{fig:mom2}-c, \ref{fig:mom2}-d, the same posterior is sampled by Anglican Particle-Gibbs (PGibbs) -- a variation of Particle-MCMC \cite{andrieu2010particle}. Anglican \cite{wood2014new} is a PPL with a syntax that (at least in the current version) promotes adding noise to all observed variables regardless if they are deterministic (e.g. $P_3$) or stochastic (e.g. $V_2$). We use normal noise variances $0.01$ and $0.2$ for observed variables $V_2$  and $P_3$ respectively.
%It can be seen that multiple noisy observations deteriorates the sampling performance.
%In the second to fourth experiments, we study the role of the size of graphical model on the performance of MCMC methods.\\



%Fig todo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
     \includegraphics[width=0.8\linewidth, height=125pt]
{plotsx/vis-col/err-vs-time__param2-shaded.pdf}
%{Figs/plots/collision/err-vs-time__param4-shaded.pdf}
      \caption{Error
as a function of wall clock time (s) for the collision model using 
MCMCs with same configurations as in Figure~\ref{fig:mom2}. 
 }
\label{fig:asymmetric}
\end{figure}





%Ex 2;  Fig 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\vspace{-0mm}
\begin{center}
\begin{tabular}{ccc}
   \hspace{-5mm} \includegraphics[width=\nnn\textwidth, height=\nnh\textwidth]{plotsx/collisionx/err-vs-time__param4-shaded.pdf} 
& \hspace{-3mm} \includegraphics[width=\nnn\textwidth, height=\nnh\textwidth]{plotsx/collisionx/err-vs-time__param20-shaded.pdf} 
& \hspace{-3mm} \includegraphics[width=\nnn\textwidth, height=\nnh\textwidth]{plotsx/collisionx/time_vs_param-errorbar.pdf}%collision/collisionTimeVsSize.png}
\vspace{-1.5mm}
\\
\hspace{-5mm} \footnotesize(a) 
& \hspace{-4mm} \footnotesize(b) 
& \hspace{-3mm} \footnotesize(c) \\
\multicolumn{3}{c}{}
\end{tabular}
\end{center}
\vspace{-8mm}
\caption{\footnotesize Wall-clock time (ns) to pass a fixed error threshold vs. the GM size, for experiments 2 to 4...TODO.}
\label{fig:mom.size}
\vspace{-4mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Ex 2;  Fig 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
\vspace{-0mm}
\begin{center}
\begin{tabular}{ccc}
   \hspace{-5mm} \includegraphics[width=\nnn\textwidth, height=\nnh\textwidth]{plotsx/conductancex/err-vs-time__param4-shaded.pdf} 
& \hspace{-3mm} \includegraphics[width=\nnn\textwidth, height=\nnh\textwidth]{plotsx/conductancex/err-vs-time__param30-shaded.pdf} 
& \hspace{-3mm} \includegraphics[width=\nnn\textwidth, height=\nnh\textwidth]{plotsx/conductancex/time_vs_param-errorbar.pdf}
\vspace{-1.5mm}
\\
\hspace{-5mm} \footnotesize(a) 
& \hspace{-4mm} \footnotesize(b) 
& \hspace{-3mm} \footnotesize(c) \\
\multicolumn{3}{c}{}
\end{tabular}
\end{center}
\vspace{-8mm}
\caption{\footnotesize Wall-clock time (ns) to pass a fixed error threshold vs. the GM size, for experiments 2 to 4...TODO....}
\label{fig:resistor}
\vspace{-4mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\textbf{Experiment 2.} %In the second experiment,
We study the role of model size on the performance of MCMC methods.
We cannot rely on \emph{MCMC convergence diagnostics} methods \cite{cowles1996markov} because they can produce misleading results in case MCMCs do not converge to the ground truth, a scenario that as we will see, for highly complex posteriors is not improbable.
Meanwhile, we cannot rely on rejection sampling to approximate the Ground truth since its performance on the studied model deteriorates drastically in dimension.
Our solution is to conduct tests on symmetric models for which the posterior ground truth can be computed manually.

We study an $n$-object collision model where 
%by assuming that %collide. But we omit the sequential dependencies of velocities s.t.\ 
all $V_i$ and $M_i$ share a same uniform prior $U(0.2, 2.2)$ and the constraint is $\sum_{i=1}^n{M_i V_i} = P$. 
The symmetry enables us to compute the posterior ground truth means  values  manually:
\begin{equation}\footnotesize
\label{e:col.GT}
M^* = V^* = \sqrt{P / n} %\left(\frac{P}{n}\right)^{0.5}
\end{equation}
Conditioned on $P = 1.5 n$, all masses $M_i$ and velocities  $V_i$ are queried. 
The tested algorithms running on the reduced-dim posterior are: 
symbolic Gibbs (see Section~\ref{sect:symbolic.gibbs}), 
baseline Gibbs (CDF computation per sample), 
%MH (Metropolis-Hastings tuned manually), 
MH automatically tuned %to reach the acceptance rate of 0.234 
after \cite{roberts1997weak})\footnote{
\label{foot:tuning}
Among 200 {\color{green} equidistant} proposal variances in %interval 
$(0, 0.1]$ %are tested by taking 100 samples using each and  
the one is chosen for which the acceptance rate is closer to 0.24.
} and rejection sampling.
Among the samplers that add noise to determinism, %{\color{green}(soften determinism(?))} 
Stan's Hamiltonian MCMC with noise (variance) parameter 
$\sigma_P = 0.05$ and Anglican's SMC with noise parameter $0.1$ are plotted.
%\footnote{Performance of PGibbs (and RDB) have been often slightly (resp. much) poorer than SMC. For readability of the plots, these results are not shown. } 
The noise parameters are chosen manually trying to maximize the convergence rate.
%Other parameters are Stan and Anglican's default setting.

By (\ref{e:col.GT}), $a_k$, the average  of $k$-th sample vector $\bvec{s}^{(k)}$,
is expected to be $\sqrt{1.5}$;   
therefore, as the overall sampling error measure, 
%$\mathbb{E}[|{\xi} - {\xi}^*|]$ 
$\mathbb{E}[|\bvec{a} - \bvec{a}^*|]$ is computed where
$\bvec{a}$ is the vector of $a_k$ (for $k$ ranges be the number of taken samples) and all entries of vector $\bvec{a}^*$ are 1.5.

For each algorithm 15 Markov chains are run and 
at consecutive time slots, their corresponding point-to-point means and standard errors are computed. 
Figure~\ref{fig:errVtimeSizeFixed}.a 
illustrates these latter results for the model of symmetric collision of 10 objects (size=10).\footnote{
Note that the dimensionality is 20 (since each object is associated with a mass and velocity random variable). %Conditioned on the total momentum, reduced-dim space is 19D and algorithms that add noise to the observation, deal with  a 21D space.
} 

We repeated the experiment for different model sizes but for the sake of space restrictions, we do not depict each plot individually.
Instead we provide a single plot that summarizes the overall behavior of each sampler w.r.t.\ the model size.
We notice that after a particular threshold, the comparative performance of the samplers remains fixed.
Therefore, in Figure ???, for each sampler the (wall-clock) time (s) to reach a sampling error threshold (average) equal to 0.3 is plotted vs the model size.\footnote{
If we pick the error threshold to be too low, many samplers would never reach it. If it is too high, samplers pass it prior to be comparatively stable. On collision model, the manual error threshold choice (around) 0.3 happens to provide a fair compromise between these two requirements.  
}%end footnote

  
%In the latter high-dimensional space, the rejection sampling has not been able to generate a particle. MH algorithms convergence rate is very low while the baseline Gibbs still perform well and the symbolic Gibbs performance is at least an order of magnitude better. Finally, Figure~\ref{fig:err-threshold-vs-size-collision} depicts the time to reach a fixed error threshold $T=0.2$ versus number of colliding objects. \\
%


\textbf{Experiment 3.} W repeat the former experiment on a model with a more complicated observed determinism. 
Consider an electrical circuit composed of $n$, $10\Omega\pm5\%$ paralleled resistors.
% with bell-shaped tolerance distributions (truncated quadratics, positive in the range $[8, 12]$). The posterior tolerance distribution is computed when 
The input current $I$ and the source voltage $V$ are observed
and the posterior distribution of resistors is required.
Therefore, the deterministic constraint is:
%Here, the observation can be stated as the following deterministic constraint:
\begin{equation} \footnotesize 
\label{e:reduced.mass}
 \frac{1}{R_1} + \ldots + \frac{1}{R_n} = c
\end{equation}
where $c = \frac{I}{V}$.
\footnote{
Note that the equations of the form (\ref{e:reduced.mass}) (generally referred to as \emph{reduced mass}) encounter in many problems in electrical, thermal, hydraulic, or mechanical engineering domains. 
}
Due to the symmetry of the problem, the posterior ground truth mean is known:
\begin{equation*}
R_i^* = \frac{n}{c} \qquad \text{ for } i = 1, \ldots, n
\end{equation*}
We assume 
priors $\pr(R_i) = U(9.5, \, 10.5)$, 
$c = \frac{n}{10.17}$,
Stan and Anglican observation noise divergences are 
0.02 and 0.07 respectively. Other settings are as before.

Figure ?? illustrates sampling error vs time (s) for a model with 10 resistors (size =10).
It can be seen that only variations of Gibbs sampling on the reduced model converge to the ground truth (that we are certain about because of symmetry argument). 
One reason for malfunctioning of MH-based algorithms may be as follows:
As can be verified by manual computations, conditioning on 
(\ref{e:reduced.mass}) leads to high degree terms in the posterior fraction. This corresponds such highly dense spots in the distribution that can act as traps for MH based algorithms.


\begin{comment}
\textbf{Experiment 4.} Here, we repeat the former experiment for a 
monotonically decreasing \emph{Dynamic Bayesian Network}. That is, a DBN with nodes $A_1$ to $A_n$
with priors: $A_1 \sim \mathcal{U}(0, b)$ and $A_t \sim \mathcal{U}(A_{t-1}, b)$ where 
$t = 2, \ldots n$ and $b=10$ is the upper bound and the deterministic constraint is $\sum A_i = c$.
This network models a power transmission line where energy loss takes place in components $A_t$.
However, many other applications, such as development of a chemical process, share similar models.  
The measurement takes place on the difference of the expected values of two parallel Markov chains to form a symmetry with ground truth \textbf{0}.
In the plotted results (Figure~\ref{fig:err-threshold-vs-size-alc}), 
the rejection sampling MC is missing since the joint is not log-concave and does not have a constant bound required for the algorithm's \emph{envelope} 
(note that $\lim_{A_{i} \rightarrow b} \frac{1}{b - A_{i}} = 0$ ).
\end{comment}


 \section{Conclusion}
\label{sect:conclusion}

We introduced piecewise algebraic graphical models that for the first time handle a large family of nonlinear deterministic restrictions, such as the physical laws, in the form of multivariate polynomial fractions. The nonlinear nature of this data leads to a new genre of posterior distributions with possible discontinuities, 
piecewise parts with nonlinear bordering hyperplanes, rapid density changes and anomalies. Our experimental results show that Gibbs sampling performs well in the presence of such unconventional densities while other Monte Carlo methods do not. We showed that in piecewise algebraic models, the univariate CDFs required for Gibbs sampling can be computed analytically and prior to the actual sampling process. Our novel sampling method, \emph{Symbolic Gibbs}, saves a significant amount of computation, improving the performance dramatically. The combination of these novel contributions should make probabilistic reasoning applicable to variety of new applications that have remained unsolvable so far.      


%*********************************************************************************************
%*********************************************************************************************
%*********************************************************************************************

% Note use of \abovespace and \belowspace to get reasonable spacing 
% above and below tabular lines. 

%\begin{table}[t]
%\caption{Classification accuracies for naive Bayes and flexible 
%Bayes on various data sets.}
%\label{sample-table}
%\vskip 0.15in
%\begin{center}
%\begin{small}
%\begin{sc}
%\begin{tabular}{lcccr}
%\hline
%\abovespace\belowspace
%Data set & Naive & Flexible & Better? \\
%\hline
%\abovespace
%Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
%Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
%Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
%Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
%Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
%Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
%Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
%\belowspace
%Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
%\hline
%\end{tabular}
%\end{sc}
%\end{small}
%\end{center}
%\vskip -0.1in
%\end{table}


% Acknowledgements should only appear in the accepted version. 

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{symbolic_gibbs_icml}
\bibliographystyle{icml2015}


{\color{blue}
//-------------------------------//
%\begin{itemize}
%\item Introducing  an expressive class of functions 
%(called PPFs for \emph{Polynomial-Piecewise Polynomial Fractions}) that facilitates inference in the presence of  linear/nonlinear algebraic deterministic constraints.
%\item Presenting an efficient inference method (called \emph{Symbolic Gibbs}) that matches well with the provided model and handles deterministic constraints.  
%\end{itemize}

%for inference in graphical models (such as Bayesian networks, Markov random fields, etc.).
%PPFs are piecewise polynomial fractions where the variable space is partitioned by polynomial inequalities.
%The key insight is that the algebraic constraints preserved PPF form after collapsing which immediately extends the application of PPFs to models with sophisticated deterministic algebraic constraints. 
We note that even in simple models such as \emph{collision example}, distributions can be piecewise fractional. For example:  
\[
\pr(V_2 \,|\, V_1) = \mathcal{U}(-2, V_1)
=
\begin{cases}
  \case{\scriptstyle (V_2 > -2), \, (V_2 < V_1)}{\frac{1}{V_1 + 2}}\\
 \otherwise{0}
 \end{cases}
\]

To show the critical role of nonlinear partitioning conditions in handling the algebraic deterministic constraints, 
let us solve equation~\ref{eq:moment-constraint} for a variable (e.g.\ $V_2$):
$V_2 = \frac{3 - M_1 V_1}{M_2}$) and modify $\pr(V_2 \,|\, V_1)$ by substituting $V_2$ with its the solution.
We will see that this substitution leads to $\pr(V_2 \,|\, V_1, P_3 = 3)$:
\begin{equation}
\label{e:fractional-constraint}
\begin{cases}
  \case{(\frac{3 - M_1 V_1}{M_2} > -2) , \, (\frac{3 - M_1 V_1}{M_2} < V_1)}{\frac{1}{V_1 +2}}\\
  \otherwise{0}
  \end{cases}
\end{equation}
Clearly, this transformation has created a piecewise function with non-linear partitioning conditions.

Recently, piecewise polynomials have attracted attention.
So far, piecewise polynomials with hyperrectangular, hyper-rhombus and linear partitioning constraints have been used in Bayesian networks \cite{shenoy2011inference,shenoy2012two,Sanner:12}.
%hyperrectangular-piecewise polynomials \cite{shenoy2011inference,shenoy2012two,Sanner:12},
%hyper-rhombus-piecewise polynomials \cite{shenoy2012two} and the most general so far, linear-piecewise polynomials \cite{Sanner:12} have been used for inference in Bayesian networks.\footnote{ By \emph{A-piecewise B} we refer to piecewise functions where the function value in each partition is in class \emph{B} and the conditions partitioning the space of variables are (conjunctions of functions) in class \emph{A}.}
These models can only handle linear dependencies and 
our model is more expressive than all of them. 
%This, for the first time, enables probabilistic inference be carried on a large domain of interesting and new  applications.  

....


\textbf{Monte Carlo methods}
%An alternative to exact inference is to approximate the joint distribution by a set of samples drawn from it using Monte Carlo methods. By the law of large numbers, this approximation is asymptotically unbiased. Using samples, the costly marginalization operation is avoided since the conditional probability $\pr(\bvec{q} \,| \, \bvec{e})$ is simply approximated by the number of samples satisfying $\bvec{q} \cap \bvec{e}$ divided by the number of samples satisfying $\bvec{e}$.
%Note that unlike other approximate methods, inference via sampling leads to asymptotically unbiased solutions i.e.\ by taking sufficient number of samples, the \emph{posterior} can be approximated by an arbitrary precision.
Approximating the posterior distribution via drawing samples from it by Monte Carlo methods 
provides an asymptotically unbiased tool that completely avoids the hassle of multiple integrations.% required in closed-form sampling.   
Three major such methods are as follows:

\emph{Rejection sampling}: In this method, to draw a sample from 
a distribution $p(\bvec{X})$, a sample $\bvec{x}$ is taken from another distribution $q(\bvec{X})$
such that $p(\bvec{X})/q(\bvec{X})$ is bound by a known constant $c$
and samples can be taken from $q(\bvec{X})$ efficiently.
The produced sample is accepted with probability $p(\bvec{x}) / c q(\bvec{x})$, 
otherwise it is rejected and the process is repeated. 

\emph{Metropolis-Hastings (MH)}:
To draw a new sample $\bvec{x}^{(t)}$ from a distribution $p(\bvec{X})$, given a previously taken sample $\bvec{x}^{(t-1)}$, 
MH takes a sample $\bvec{x}'$ from a symmetric \emph{proposal density} $q(\bvec{X} |\, \bvec{x}^{(t-1)})$. 
%from which samples can be taken efficiently 
%(often an isotropic \emph{Gaussian} centered at $\bvec{x}^{(t-1)}$). 
With probability $\min \big(1, p(\bvec{x}')/p(\bvec{x}^{(t-1)}) \big)$, 
$\bvec{x}'$ is accepted as the next sample ($\bvec{x}^{(t)} \leftarrow \bvec{x}'$), otherwise, $\bvec{x}^{(t)} \leftarrow \bvec{x}^{(t-1)}$. 
Choosing a good \emph{proposal} is problem-dependent and requires tuning. 


\emph{Gibbs sampling}:
Gibbs is a robust sampling tool in the sense that it does not require known bounds, tuned proposal densities or parameters.
Drawing a sample for $\bvec{X} = (X_1, \ldots, X_N)$ takes place in $N$ steps.
In the $i$-th step, $X_i$ is sampled conditioned on the last realization of the others:
$x_i \sim \pr(X_i \,|\, \bvec{x}_{-i})$. 
To perform this task, the following univariate \emph{Cumulative Distribution Function} (CDF)
is computed by equation~(\ref{e:cdf}) and samples are taken by inverse transform sampling. 
{\footnotesize
\begin{equation}
\label{e:cdf}
F(X_i  \,|\, \bvec{x}_{-i}) 
\propto
\int_{-\infty}^{X_i} \!\!\!\! \pr(X_i = t, \bvec{x}_{-i}) \, d  t
\end{equation} 
}

.............

{\color{green}As mentioned, conditioning on deterministic constraints lead to posterior masses positioned  on submanifolds and transformations that are required for dimension reduction
often end in distributions which are highly piecewise and multimodal.
This class of distributions is intrinsically different from the smooth, bell-shaped distributions often studied in the literature. }

............

We tested the algorithms supported by Anglican:
 sequential
Monte Carlo (SMC) \cite{wood2014new},
Particle-Gibbs (PGibbs) (a variation of Particle-MCMC\cite{andrieu2010particle}) 
and a variation of MH introduced in \cite{wingate2011lightweight} and named \emph{random database} (RDB) in \cite{wood2014new}.
Due to existence of multiple noisy observations, in terms of size and accuracy of the effective samples, the performance of all three algorithms were low. The best performance was due to RDB which is plotted. 
 

}%end blue
{
\color{red}
[Cobb2005] B. R. Cobb and P. P. Shenoy. Nonlinear deterministic relationships in Bayesian networks.
In L. Godo, editor, Symbolic and Quantitative Approaches to Reasoning with
Uncertainty: 8th European Conference, ECSQARU 2005, Lecture Notes in Artificial
Intelligence 3571, pages 27–38. Springer, Berlin, 2005.

DETERMINISTIC CONDITIONALS?

}
\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  

