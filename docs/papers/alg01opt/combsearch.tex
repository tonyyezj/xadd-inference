\section{Combinatorial Search for 0--1 Loss}
\label{cha:combinatorialsearch}

This section introduces the idea behind the combinatorial search
approach, which is illustrated by Figure~\ref{fig:cs_intro}. In short,
we observe that the hyperplane passing through a set of $D$ points
maps to an equivalance class of hyperplanes all with the same 0--1
loss.  This suggests a simple combinatorial search algorithm to
enumerate all $\binom{N}{D}$ hyperplane equivalence class representatives 
to find the one with minimal 0--1 loss.

\begin{figure}[here]
\includegraphics[width=0.50\textwidth]{images/fig41_intro.eps}
\caption{ \footnotesize This plot illustrates the idea behind
  the combinatorial search approach. {\bf H} is the
  optimal decision hyperplane to separate the two classes. {\bf H'} is
  obtained from {\bf H} 
  so that it goes through the two nearest points A and B
  without changing the class assignments or loss value.}
\label{fig:cs_intro}
\end{figure}

To formulate the approach discussed above, we write 
the 0--1 loss for $\xi$ in non-homogenous notation as:
\begin{equation}\label{eq:loss}
L(w_0,\w) = \sum_{i=1}^N \mathbb{I} [w_0 + t_i (\w^T \xi) < 0].
\end{equation}
%In this definition, the non-strict inequality inside the
%indicator function is replaced by a strict inequality.  The only
%difference this makes is in the way how to treat points lying directly
%on the decision hyperplane: before these are deemed to be
%misclassified, now correctly classified. As it is expected that there
%are none or only a few points lying on the decision hyperplane, this
%difference is negligible.
%
%A decision hyperplane that separates the two classes is now given by
%$w_0 + \w^T\boldsymbol{x} = 0$, and the optimal solution $\w_{opt} =
%(w_0^*, \w^*)$ consists of the bias and the weight vector that
%minimizes the 0--1 loss function $L(w_0, \w)$ above. If the bias term
%$w_0$ is zero, the decision hyperplane must go through the origin,
%which is very rare in practice. Thus, it is reasonable to assume that
%$w_0 \not= 0$. 
As noted previously, if both $w_0$ and $\w$ are scaled by $1/|w_0|$, the
solution is unchanged (assuming $w_0 \neq 0$).  Specifically, there are two
cases: first, if $w_0>0$, then the loss function is:
\begin{align*}
L(w_0,\w) &= \sum_{i=1}^N \mathbb{I} [w_0 + t_i (\w^T \xi) < 0] \\
&=\sum_{i=1}^N \mathbb{I} [\frac{w_0}{w_0} + \frac{t_i}{w_0}\w^T \xi < 0]  \\
&= \sum_{i=1}^N \mathbb{I} [1 + t_i\w'^T \xi < 0] = L(1, \w')
\end{align*}
where we have defined $\w' = \frac{1}{w_0}\w$. 

Second, if $w_0 < 0$, the loss function is 
\begin{align*}
L(w_0,\w) &= \sum_{i=1}^N \mathbb{I} [w_0 + t_i (\w^T \xi) < 0] \\
&=\sum_{i=1}^N \mathbb{I} [\frac{w_0}{-w_0} + \frac{t_i}{-w_0}\w^T \xi) < 0]  \\
&= \sum_{i=1}^N \mathbb{I} [-1 - t_i \w'^T \xi) < 0] 
= L(-1, -\w').
\end{align*} 

The equation of the decision hyperplane is the same in both cases 
($w_0 + \w^T\x = 0 \;  \Leftrightarrow \; 1 + \w'^T \x = 0)$
and the loss
function is either $L(1, \w')$ or $L(-1, -\w')$, i.e., the bias term
is now either $1$ or $-1$. As shall be
seen next, this fact is critically important for the purpose of the
combinatorial search approach. As the initial discussion 
pointed out, to find the optimal solution, it suffices to
check all hyperplanes that go through $D$ points of the training
dataset and find the one that has minimum 0--1 loss. So, assuming
$\x_1, \dots, \x_D$ are (any) $D$ distinct
data points from the training dataset, then for the combinatorial
search to work, the two following tasks must be solved:
%\begin{enumerate}
%\item 
(1) Find the weight vector $\w'=(w'_1, \dots, w'_D)^T$ of the
  decision hyperplane that goes through these $D$ selected points.
%\item 
(2) Calculate the 0--1 loss value corresponding to $\w'$.
%\end{enumerate}

For the first task, because the hyperplane goes through the given $D$
data points, at each point the hyperplane equation must be
satisfied. So,
$$ 1+ \w'^T\xi = 0, \quad\quad \text{for } i = 1, 2, \dots, D,$$
which is written in matrix form as 
$A \w' = -\boldsymbol{1},$
where $A =(\x_1\; \x_2\; \dots \; \x_D)^T$ and $\boldsymbol{1}$ is the unit vector in $\R^D$.
This linear matrix equation can be easily solved to get a particular solution $\w'$ using LU decomposition. Here, one sees that if the bias
term $w_0$ was still present, the above equation would
be underdetermined. 

Now, with $\w'$ specified, the second task becomes easy, as the 0--1
loss value is obviously the smaller value of $L(1,\w')$ and $L(-1,
-\w')$. Thus, if $L(1,\w') \leq L(-1,-\w')$, the 0--1 loss value
corresponding to decision hyperplane $1+\w'^T\x = 0$ is
$L(1,\w')$, and the solution vector (including bias and weights) is
$(1, \w')$, otherwise, the 0--1 loss value is $L(-1,-\w')$, and the
solution vector is $(-1,-\w')$. Note that generally $N >> D$, so the class of the $D$ selected points can be assigned to the most frequent class (one could use a separating hyperplane method to do better but the gain is insignificant).

The above discussion represents all necessary knowledge for the
combinatorial search approach and it is now possible to build
algorithms based on the foundation presented here.  We present
two variants: one provably optimal and one approximate.

\noindent\emph{Prioritized Combinatorial Search (PCS)}: This algorithm
exploits the fact that combinations of data points lying closer to an
initial approximated decision hyperplane (e.g., given by an SVM) are
more likely to produce the optimal hyperplane than combinations of
points lying far away. Algorithm~\ref{alg:cs.prioritized} captures this idea by considering
combinations of $D$ points in the increasing order of their distance
to the approximated decision hyperplane, where the distance of a \emph{set}
of points to a hyperplane is the minimal distance of points in the set.  PCS
can find an optimal solution in $O(D^3 \binom{N}{D})$ 
time ($\binom{N}{D}$ iterations, each requires $D^3$ time to solve the linear matrix equation),
which can be much more efficient than \BB\ for small $D$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t!]
%\vspace{-3mm}
\caption{Prioritized Combinatorial Search (PCS)}
\label{alg:cs.prioritized}
{\footnotesize 
\begin{algorithmic}%[1]
\INPUT {Training data $(\X,\t)$}
\OUTPUT {Weights $\w^*$ minimizing 0--1 loss}
\FUNCTION {{\sc Find-Optimal-01Loss-PCS}($\X, \t$)} %\COMMENT{returns $\w^*$}
\STATE $\w^* \gets \w^*_{\mathit{SVM}}$ from SVM solution for $(\x,\t)$
\STATE $loss_{min} \gets$ 0--1 loss implied by $\w^*$
\STATE $\boldsymbol{i} \gets$ indices of $\xi$ ordered by $|{\w^*}^T\x_k|$, for $k=1..N$.
\STATE $\p \gets [1,2,\dots,D]$
\COMMENT{first combination of $D$ points}
\WHILE{$\p \not= \emptyset$}
   \STATE $(\w,loss) \gets$ {\sc Get-Solution}($\p$)
   \IF {$loss < loss_{min}$}
      \STATE $(\w^*, loss_{min}) \gets (\w, loss)$
   \ENDIF
   \STATE $\p \gets $ next combination of ${N \choose D}$, or $\emptyset$ if no more.
\ENDWHILE
\STATE {\bfseries return} $\w^*$
\STATE
\FUNCTION{{\sc Get-Solution}($\p$)}
   \STATE $A \gets (x_{i[p_1]} \, x_{i[p_2]} \, \dots \, x_{i[p_D]})^T$
   \STATE $\w' \gets$ a particular solution of $A \w' = -\boldsymbol{1}$
   \IF {$L(1,\w') \leq L(-1,-\w')$}
      \STATE $\w \gets (1, \w')$
      \STATE $loss \gets L(1,\w')$
   \ELSE
      \STATE $\w \gets (-1, -\w')$
      \STATE $loss \gets L(-1,-\w')$
   \ENDIF
   \STATE {\bfseries return} $(\w, loss)$ \COMMENT{corresponding to $\p$}
\ENDFUNCTION
\ENDFUNCTION
\end{algorithmic}}
%\vspace{-4mm}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\MYCOMMENT

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tp!]
\vspace{-3mm}
\caption{
Combinatorial Search Approximation (CSA).\hfill \; \\
%\text{\hspace{2.1cm}} $Input$: Dataset of training data points $ \x$, their labelled class targets $\t$. \\
%\text{\hspace{2.1cm}} $Output$: Optimal weight vector $\w^*$ minimizing 0--1 loss.
}
\label{alg:cs.approximation}
{\footnotesize 
\begin{algorithmic}[1]
\Function{Approximate-01Loss-Solution}{$\X, \t$} \Comment{returns $\w^*$}
\STATE $\w^* \gets$ approximated weight vector given by an SVM
\STATE $\boldsymbol{i} \gets$ indices of points ordered by $|{\w^*}^T\x_k|$, for $k=1, \dots, N$.
\STATE $(\p, \w^*, loss_{min}) \gets$ \Call{Get-Best-Initial-Solution}{$30$}
\Loop
   \For {$k=1$ to $N$}
      \If {$k \not\in \p$}
         \For {j=1 to D} 
            \STATE $\p' \gets \p$
            \STATE $p'_j \gets k$
            \Comment{replace $j-$th component by $k$}
            \STATE $(\w, loss) \gets$ \Call{Get-Solution}{$\p'$}
            \If {$loss < loss_{min}$}
               \STATE $(\p, \w^*, loss_{min}) \gets (\p', \w, loss)$
               \STATE {\bf go back to step 5}
            \EndIf
         \EndFor
      \EndIf
   \EndFor
   \STATE \Return $\w^*$
 \EndLoop
\STATEx
\Function{Get-Best-Initial-Solution}{$N'$} \Comment{returns $(\p, \w, loss)$}
   \STATE $loss \gets +\infty$
   \STATE $\p_t \gets [1,2,\dots,D]$
   \Comment{initialize the first combination of $D$ points}
   \While{$\p_t \not= \emptyset$}
      \STATE $(\w_t, loss_t) \gets$ \Call{Get-Solution}{$\p_t$}
      \If {$loss_t < loss$}
         \STATE $(\p, \w, loss) \gets (\p_t, \w_t, loss_t)$
      \EndIf
      \STATE $\p_t \gets $ next combination of ${N' \choose D}$, or $\emptyset$ if there is no more.
   \EndWhile
   \STATE \Return $(\p, \w, loss)$
\EndFunction
\STATEx
\EndFunction
\end{algorithmic}}
\vspace{-4mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ENDMYCOMMENT

\noindent \emph{Combinatorial Search Approximation (CSA)}: Rather than
systematically enumerating all combinations as in prioritized search,
we start from an initial ``best'' combination of $D$ points near an
approximated decision hyperplane (e.g., given by an SVM), then at each
iteration, we swap two points $(\x_k, \x_j)$ in/out of the
current combination. The algorithm stops when it cannot find any more
points to swap.  We do not present the full algorithm here due to
space limitations but note that it is a slight variation on Algorithm~\ref{alg:cs.prioritized}.
