\section{Related Work and Conclusions}
%\section{Conclusions}
\label{cha:conclusions}

\MYCOMMENT
% See original background content
In short, others have looked at robust solutiosn, but no one
has looked at directly optimizing 0--1 loss with one
exception.
\ENDMYCOMMENT

%We have explored a variety of algorithms for direct 0--1 loss
%optimization and have shown one of these approaches --- smooth
%0--1 loss approximation (SLA) --- is fastest and performs as well as
%state-of-the-art classification algorithms and often better in the
%presence of outliers.  
While other works have investigated classification methods that are
robust to outliers, including t-logistic regression \cite{Ding},
robust truncated hinge loss \cite{wu07}, SavageBoost
\cite{lossdesign}, and \cite{collobert} (which shows that a non-convex
piecewise linear approximation of 0--1 loss can be
efficiently optimized), in this work we have chosen to \emph{directly}
optimize 0--1 loss using search-based techniques (some with strong
finite time guarantees) in contrast to the randomized descent approach
of~\cite{ling}.  Empirical results demonstrate the
importance of 0--1 loss for its robustness properties while providing
evidence that it can be effectively optimized in practice.

\MYCOMMENT

\subsection{Future Work}
\label{sec:concl.futurework}

Incrementally maintain half-spaces.

Parallelization.

Use subset of points in SLA close to current boundary... other
points unlikely to change from small shift in boundary.

Currently, the first process of the SLA algorithm uses a modified
gradient descent mechanism. However, there are faster optimization
methods that can be used, such as the pseudo-second order
Levenberg-Marquardt method \cite{Marquardt}, or the trust region
Newton's method \cite{Steihaug}.

Boosting?

\ENDMYCOMMENT
